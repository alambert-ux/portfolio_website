---
date: "2017-10-31T21:28:43-05:00"
description: "How Do Airline Passengers Feel About Their Providers?"
draft: false
image: pic11.jpg
keywords: ""
slug: rockerfeller
title: Rockerfeller

---

# The Problem

Airline carriers receive a high volume of public Tweets daily, which in the most part they need to interact with or address directly both to ensure good customer experience and avoid public humiliation. Tweets vary from positive "thank you" messages to complaints and accusations of "lost luggage", "delayed flights" and "unfair fees". Analysing this data in real-time is crucial for Airlines' customer service teams to address complaints and negative publicity proactively and positively, and make happy passengers feel valued and recognised. These two roles must be balanced, however, and as such it is more important that airlines capture negative reviews with accuracy than it is that they catalogue positive reviews. We discuss what this means for true positive and true negative rates, i.e. sensitivity and specificity. In this case, it is evident that specificity, the true negative rate, must be paramount as we create a dictionary-based classifier to help Airlines to target and address negative reviews online. 

# The Data

I employ a key dataset of Tweets associated with the public Twitter accounts of a number of prominent US-operating Airlines, scraped directly from Twitter in 2019. 

# The Solution

### How Should Airlines Classify the Sentiment of their Tweets? 

**First, the required packages are loaded...**
```{r , results=FALSE, message=FALSE, warning=FALSE}

#LOAD ALL THE NECESSARY PACKAGES FOR QUESTIONS A, B AND C:
library(arm)
library(boot)
library(pROC)
library(mfx)
library(ggplot2)
library(lme4)
library(data.table)
library(Metrics)
library(sjmisc)
library(sjPlot)
library(margins)
library(quanteda)
library(glmnet)
library(ggpubr)

```

**Next, I clean the dataset and perform a preliminary investigation of word usage...**
```{r , results=FALSE, message=FALSE, warning=FALSE}
#LOADING THE DATASET (tweets) FOR FURTHER ANALYSIS
#Load in the correct data files and review using summary(), to understand their structures:
load(here::here('data','tweets.Rda'))

summary(tweets)

#CREATING A CORPUS OF TWEETS: 
speechCorpus <- corpus(tweets$text, docvars = tweets)

#CONVERTING THE CORPUS INTO A DOCUMENT-TERM MATRIX (DTM) USING THE dfm() FUNCTION
dfm.tweets <- dfm(speechCorpus, tolower=T, remove_numbers=T, remove_punct=T, remove = stopwords("en"))
dim(dfm.tweets)
#THIS SHOWS THAT THERE ARE 12,303 UNIQUE WORDS (THE COLUMN DIMENSION OF THE DOCUMENT-TERM MATRIX), CONTAINED IN THE 11,541 RECORDED TWEETS (THE ROW DIMENSION OF THE DOCUMENT-TERM MATRIX)

#I DECIDE TO REMOVE RARE WORDS USED ONLY IN A SINGLE TWEET, AS THEY ARE UNLIKELY TO BE INFORMATIVE ON AGGREGATE
doc_freq <- docfreq(dfm.tweets)
dfm.tweets <- dfm.tweets[,doc_freq>1]
dim(dfm.tweets)
#THIS SIGNIFICANTLY REDUCES THE NUMBER OF UNIQUE WORDS, FROM 12,303 TO JUST 5,213. THESE ARE WORDS USED IN MORE THAN 1 TWEET, AND THEREFORE WHICH CAN BE USED TO INFORM GENERALISED ASSOCIATIONS BETWEEN WORDS AND NEGATIVE OR POSITIVE SENTIMENT

#I NOW INVESTIGATE WORD USAGE MORE CLOSELY, BOTH BETWEEN AIRLINES AND IN TERMS OF POSITIVE AND NEGATIVE SENTIMENT
#THE TF-IDF WEIGHTED WORD FREQUENCIES ARE CREATED, BASED ON THE ORIGINAL dfm.tweets DOCUMENT-TERM MATRIX: THIS TF-IDF WEIGHTED MATRIX WILL BE USED FOR ALL SUBSQUENT ANALYSIS FOR SECTION G
#THIS IS OF COURSE BUILT UPON dfm.tweets, AND SO ONLY INCLUDES WORDS USED IN MORE THAN 1 TWEET
dfm.tweets.tfidf <- dfm_tfidf(dfm.tweets)

#REVIEWING THE 10 MOST USED WORDS IN THE CORPUS AS A WHOLE
textstat_frequency(dfm.tweets.tfidf, force=TRUE)[1:10,]

```

**Now I identify words associated with Negative and Positive Sentiment, plotting these alongside eachother in order of frequency...**
```{r , results=FALSE, message=FALSE, warning=FALSE}
#I GROUP THE TWEETS BY SENTIMENT AND FIND THE 15 MOST USED WORDS ASSOCIATED WITH EACH SENTIMENT
by.sentiment <- textstat_frequency(dfm.tweets.tfidf,15,groups="sentiment", force=TRUE)
by.sentiment

#WHICH COMPARES WORD USEAGE BETWEEN POSITIVE AND NEGATIVE SENTIMENT TWEETS

by.sentiment$group <- ifelse(by.sentiment$group==1,"Negative Sentiment","Positive Sentiment")

#ADJUSTMENT OF RELEVANT WORDS: REMOVED  WORDS ARE AIRLINE NAMES, AND EXPRESSIONS WHICH ARE  NEUTRAL: NEITHER POSITIVE NOR NEGATIVE IN NATURE, LARGELY ASSOCIATED WITH SPECIFIC AIRLINES, AIRPORT CODES, OR OTHER JARGON SUCH AS "time","flying" and "crew", WHICH OFFER LITTLE INFORMATION IN TERMS OF WHICH WORDS ARE ACTUALLY ASSOCIATED WITH EITHER POSITIVE OR NEGATIVE SENTIMENT
#I ATTEMPT TO REMOVE AS FEW WORDS AS POSSIBLE, IN ORDER NOT TO BIAS THE OUTCOME BY MAKING PRESUMPTIONS OF NEUTRALITY/POSITIVE/NEGATIVE SENTIMENT WORDS
dfm.tweets.tfidf.2 <- dfm_remove(dfm.tweets.tfidf,c("@united","@usairways","@americanair","@southwestair","@jetblue","@virginamerica","t.co","http","amp","flightled","w","airline","flying","flight","can","plane","one","flights","back","gate","fly"))
by.sentiment.2 <- textstat_frequency(dfm.tweets.tfidf.2,15,groups="sentiment", force=TRUE)

#THIS IS A PLOT WHICH COMPARES WORD USEAGE BETWEEN POSITIVE AND NEGATIVE SENTIMENT TWEETS BUT ADJUSTS FOR REDUNDANT WORDS/EXPRESSIONS
by.sentiment.2$group <- ifelse(by.sentiment.2$group==1,"Negative Sentiment","Positive Sentiment")
ggplot(by.sentiment.2,aes(x=frequency,y=reorder(feature,frequency))) + facet_wrap(~group) + geom_point() + ylab("") + xlab("Frequency (TF-IDF)")
#AS THIS SHOWS, "service", "just", "customer" ARE EXPRESSIONS WHICH ARE USED ACROSS BOTH POSITIVE AND NEGATIVE TWEETS, AND THUS ARE NOT DEFINITIVELY ASSOCIATED WITH PARTICULAR SENTIMENT
```

**The figure below shows a frequency comparison for words used most in Tweets classified as Positive and Negative, omitting redundant words...**
```{r echo=TRUE, fig.show=TRUE, warning=FALSE, results=TRUE}
ggplot(by.sentiment.2,aes(x=frequency,y=reorder(feature,frequency), geom_point(~group),colour=group)) + geom_point() + ylab("") + xlab("Frequency (TF-IDF)") + ggtitle("WORD FREQUENCY | DIFFERING WORD USAGE BY SENTIMENT ") + labs(color = "Sentiment")
```
  
  This plot shows:

*A comparison of the Top 15 Words by (TF-IDF weighted) Frequency used across Positive and Negative Sentiment Tweets. Words in the Top 15 frequencies for both groups appear on the same horizontal line.*

Of 11,541 sampled tweets, 2,363 are positive and 9,178 negative in sentiment. This figure indicates words most associated with each sentiment: the largest words depicting those with the highest frequency in each group. This weighting scheme is applied to reflect the importance of each word to tweets in their respective group, achieved by assigning a value which increases proportionally to the frequency of use in each tweet, but is offset by the number of tweets which contain the word, adjusting for natural differences in occurrence. Thus, “thank(s)”, “great”, “love” and “awesome” are most associated with positive sentiment; whilst “get”, “cancelled”, “hours”, “hold”, and “now” are most associated with negative sentiment. There is some overlap between more ambiguous words such as “service”, but each are more strongly associated with negative sentiment. Positive words are prominent because there is a large disparity between the frequencies of common positive words, with “thank(s)” having a frequency at least 2x higher than the 3rd most used word, “great”. Contrastingly, there are no stand-out words associated with negative sentiment, which all have a high relative frequency.

```{r , results=FALSE, message=FALSE, warning=FALSE}
#I NOW IDENTIFY THE 15 MOST USED WORDS ASSOCIATED WITH EACH SENTIMENT, POST-REMOVAL OF REDUNDANT EXPRESSIONS. THIS IS USED TO AID MY ANALYSIS WITH MORE DETAILED WEIGHTED WORD FRQUENCY COMPARISON
by.sentiment.2
```

**I now plot the most used words in Tweets about each airline as a wordcloud, where the size of the word and its centrality reflects how widely used it is in relation to the corresponding Airline...**
```{r , results=FALSE, message=FALSE, warning=FALSE}
#I REPEAT THE DESCRIPTION AND COMPARISON PROCESS ABOVE, BUT FOR THE 6 AIRLINES IN THE SAMPLE
#I GROUP THE TWEETS BY AIRLINE AND FIND THE 15 MOST USED WORDS ASSOCIATED WITH EACH AIRLINE
by.airline <- textstat_frequency(dfm.tweets.tfidf,15,groups="airline", force=TRUE)

dfm.tweets.tfidf.3 <- dfm_remove(dfm.tweets.tfidf,c("@united","@usairways","@americanair","@southwestair","@jetblue","@virginamerica","united","virgin","jetblue","southwest","#unitedairlines","#usairways","t.co","fll","phl","lga","mco","#americanairlines","SWA","sw","SW","swa","ua","iad","ewr","ord","las","bos","lt","gt","la","clt","dca","bwi","atl","lax", "sfo","dfw","jfk","#jetblue","http","amp","flightled","iah","pdx","vx","dc","rt","va","flightr","phx","flt","ur","bna","aa","jb"))
by.airline.2 <- textstat_frequency(dfm.tweets.tfidf.3,15,groups="airline", force=TRUE)

```

```{r echo=TRUE, fig.show=TRUE, warning=FALSE, results=TRUE, fig.height=10, fig.width=10, fig.align='centre'}
speechCorpus_small <- corpus_subset(speechCorpus, airline %in% c("United","JetBlue","American","US Airways","Virgin America","Southwest"))
dfm.tweets.3 <- dfm(speechCorpus_small, groups="airline",tolower=T,remove_numbers=T,remove = stopwords("en"),remove_punct=T)
dfm.tweets.3 <- dfm_remove(dfm.tweets.3,c("@united","@usairways","@americanair","@southwestair","@jetblue","@virginamerica","united","virgin","jetblue","southwest","#unitedairlines","#usairways","t.co","fll","phl","lga","mco","#americanairlines","SWA","sw","SW","swa","ua","iad","ewr","ord","las","bos","lt","gt","la","clt","dca","bwi","atl","lax", "sfo","dfw","jfk","#jetblue","http","amp","flightled","iah","pdx","vx","dc","rt","va","flightr","phx","flt","ur","bna","aa","jb"))
textplot_wordcloud(dfm.tweets.3,max_size=3,max_words=300,comparison=T)
```

This plot shows:

*Word usage between Tweets about different Airlines. Each coloured region reflects tweets about a different airline. The size of the word corresponds with (TF-IDF weighted) frequency in each group of tweets: the larger the word, the higher its tweet-level frequency in that group. Words are coloured according to the group of tweets in which they have the highest relative frequency.*

This demonstrates notable differences in word use associated with each airline, though “flight” is most frequent in all cases. The Top 15 words associated with United have the highest average word frequency, and so are particularly representative of sentiment towards it. Contrastingly, Virgin shows a relatively low and homogeneous frequency of words, suggesting associated words are more varied and not as important in characterising Virgin tweets as words for other airlines, which occur in far higher frequencies.

The relative prominence of individual words across airlines in the wordcloud, in which Virgin’s “site”, US Airways’ “hold” and Southwest’s “cancelled” are each considerably larger, is due to disproportionately higher weighted frequencies relative to other high-frequency words associated with the airline. 

**Presenting this in a more quantifiable format, I now plot the word frequencies for each airline on a single frequency plot...**
```{r echo=TRUE, fig.show=TRUE, warning=FALSE, results=TRUE}
ggplot(by.airline.2,aes(x=frequency,y=reorder(feature,frequency), geom_point(~group),colour=group)) + geom_point() + ylab("") + xlab("Frequency (TF-IDF)") + ggtitle("WORD FREQUENCY | DIFFERING WORD USAGE ACROSS AIRLINES") + labs(color = "Airline")
```

This plot shows:

*Direct Comparison of the Top 15 Words by (TF-IDF weighted) Frequency used across Tweets associated with each Airline. Words in the Top 15 frequencies for multiple groups appear on the same horizontal line*

This figure reveals that positive sentiment such as “thanks” are strongly associated with United and JetBlue and “love” and “[web]site” associated with Virgin: though whether this is associated with particular sentiment is unclear. Conversely, words also highly associated with negative sentiment (see Figure C2), such as “cancelled”, “hold” and “service”, are strongly associated with American and Southwest, and US Airways and United, respectively. Unlike any other airline, words associated with Virgin are exclusively positive or neutral, suggesting it receives consistently positive engagement, despite relatively low importance of associated words.

```{r  , results=FALSE, message=FALSE, warning=FALSE}
#I NOW FIND THE 15 MOST USED WORDS ASSOCIATED WITH EACH SENTIMENT, POST-REMOVAL OF REDUNDANT EXPRESSIONS: THIS IS USED TO CREATE (TABLE C1) IN THE MAIN REPORT, BY WHICH THE 5 MOST USED WORDS FOR EACH AIRLINE ARE TABULATED
by.airline.2
```

# Diving Deeper

### Now that I have evaluated the most common words associated with manually classified positive and negative Tweets, I create a dictionary to make these positive and negative classifications myself...

When selecting words for a Dictionary to classify positive and negative sentiment, I attempted to fulfil the following criteria, in order of priority:

1. No overlap of high-frequency words between sentiments
2. Maximum frequency associated with the highest frequency words in each group
3. Minimal “noise” from redundant or contextually neutral words

This criteria is designed to maximise the accuracy of dictionary-based classification, categorising words exclusively associated with positive or negative sentiment, and in high frequency. This means if a Dictionary word is present, there is a maximal probability not only that the tweet associated with a given sentiment, but a considerably lower probability it is associated with the opposite sentiment. Beginning with a TF-IDF matrix excluding redundant terms, such as airport codes, “flight”, “@” for airlines and any irrelevant “#”, the Top 15 positive and negative sentiment word frequencies are plotted to identify words most frequently associated with each group and any overlap in high frequency word usage. Any words present in the Top 15 most frequent words in both groups are eliminated. This is repeated until the “no-overlap” criterion is fulfilled, generating the 15 highest frequency words associated exclusively with each sentiment, utilised to define the Dictionary.

**Here I build a dictionary-based classifier to label Tweets as positive or negative in sentiment depending on the words they contain...**
```{r , results=FALSE, message=FALSE, warning=FALSE}
#THE DOCUMENT-TERM MATRIX I INTEND TO USE FOR CREATION OF A DICTIONARY OF NEGATIVE AND POSITIVE WORDS DESCRIBING AIRLINES IS AS BEFORE, BUT EXCLUDING ALL THE PREVIOUSLY IDENTIFIED REDUNDANT WORDS (BOTH THOSE REMOVED IN SECTION G.1 AND SECTION G.2): 
dfm.tweets.tfidf.final.1 <-  dfm_remove(dfm.tweets.tfidf,c("@united","@usairways","@americanair","@southwestair","@jetblue","@virginamerica","t.co","http","amp","flightled","w","airline","flying","flight","can","plane","one","flights","back","gate","fly","fll","phl","lga","mco","#americanairlines","SWA","sw","SW","swa","ua","iad","ewr","ord","las","bos","lt","gt","la","clt","dca","bwi","atl","lax", "sfo","dfw","jfk","#jetblue","http","amp","flightled","iah","pdx","vx","dc","rt","va","flightr","phx","flt","ur","bna","like","aa","jb"))

#IN MY DICTIONARY, I INCLUDE ONLY THE TOP 15 'POSITIVE' WORDS AND THE TOP 15 'NEGATIVE WORDS' 
#THE TERMS ALREADY REMOVED IN THIS DATASET, ARE EXCLUSIVELY NEUTRAL TERMS, MEANING THAT THE TOP 15 POSITIVE AND NEGATIVE WORDS ARE IN FACT POSITIVE OR NEGATIVE, AND NOT SIMPLY NEUTRAL WORDS WHICH HAPPEN TO OCCUR WITH HIGH FREQUENCY IN ONE TYPE OF TEXT
#THIS MEANS THAT DEFINING A DICTIONARY ON THE BASIS OF THE TOP 15 (NON-NEUTRAL) WORDS ASSOCIATED WITH POSITIVE SENTIMENT, AND TOP 15 WORDS ASSOCIATED WITH NEGATIVE SENTIMENT, IS LIKELY TO RESULT IN QUALITY CLASSIFICATION OF UNSEEN TWEETS
sentiment.dictionary.1 <- textstat_frequency(dfm.tweets.tfidf.final.1,15,groups="sentiment", force=TRUE)

#WORDS SELECTED FOR THE DICTIONARY MUST HAVE NO OVERLAP, SO I PLOT THE POSITIVE AND NEGATIVE SENTIMENT WORD FREQUENCIES TO ENSURE THAT THIS IS THE CASE, MEANING THE IDENTIFIED WORDS ARE NOT ONLY IN HIGH FREQUENCY IN THEIR TYPE OF DOCUMENT, BUT THAT THEY ARE NOT ALSO IN HIGH FREQUENCY IN THE OTHER TYPE OF DOCUMENT, WHICH IS LIKELY TO RESULT IN A HIGH ERROR RATE IN CATEGORISATION
sentiment.dictionary.1$group <- ifelse(sentiment.dictionary.1$group==1,"Negative Sentiment","Positive Sentiment")
ggplot(sentiment.dictionary.1,aes(x=frequency,y=reorder(feature,frequency))) + facet_wrap(~group) + geom_point() + ylab("") + xlab("Frequency (TF-IDF)")

#THIS PLOT SHOWS THAT THERE IS SOME OVERLAP BETWEEN THE WORDS "service", "just", "customer", "help" and "time" IN THE TWO GROUPS, AS WELL AS SOME REMAINING 'NEUTRAL' WORDS: I THEREFORE ELIMINATE THESE TERMS FROM THE MATRIX AND REPEAT, ITERATIVELY ENSURING THAT THERE IS NO OVERLPA IN HIGH FREQUENCY WORDS BETWEEN  SENTIMENTS, TO CREATE A DICTIONARY BASED ON WORDS STRONGLY ASSOCIATED WITH EXCLUSIVELY ONE TYPE OF SENTIMENT
dfm.tweets.tfidf.final.2 <-  dfm_remove(dfm.tweets.tfidf,c("@united","@usairways","@americanair","@southwestair","@jetblue","@virginamerica","t.co","http","amp","flightled","w","airline","flying","flight","can","plane","one","flights","back","gate","fly","fll","phl","lga","mco","#americanairlines","SWA","sw","SW","swa","ua","iad","ewr","ord","las","bos","lt","gt","la","clt","dca","bwi","atl","lax", "sfo","dfw","jfk","#jetblue","http","amp","flightled","iah","pdx","vx","dc","rt","va","flightr","phx","flt","ur","bna","like","aa","jb","service","just","customer","help","time","get","us"))
sentiment.dictionary.2 <- textstat_frequency(dfm.tweets.tfidf.final.2,15,groups="sentiment", force=TRUE)

sentiment.dictionary.2$group <- ifelse(sentiment.dictionary.2$group==1,"Negative Sentiment","Positive Sentiment")
ggplot(sentiment.dictionary.2,aes(x=frequency,y=reorder(feature,frequency))) + facet_wrap(~group) + geom_point() + ylab("") + xlab("Frequency (TF-IDF)")
```

  **Using the dictionary I have created, I now plot a full weighted word frequency comparison between positive and negative sentiment words used in the dictionary, as well as a list of the top 10 positive and top 10 negative words across all airlines' tweets...**
```{r fig.show=TRUE, fig.height=10, fig.width=10, fig.align='centre'}
ggplot(sentiment.dictionary.2,aes(x=frequency,y=reorder(feature,frequency), geom_point(~group),colour=group)) + geom_point() + ylab("") + xlab("Frequency (TF-IDF)") + ggtitle("DICTIONARY WORDS | WORDS UNIQUELY ASSOCIATED WITH EACH SENTIMENT") + labs(color = "Sentiment")
#THERE IS NOW NO OVERLAP IN THE WORDS MOST ASSOCIATED WITH POSITIVE OR NEGATIVE SENTIMENT, AND THUS THE TOP 15 WORDS IN TERMS OF FREQUENCY, FOR EACH TYPE OF SENTIMENT, ARE UNIQUELY ASSOCIATED WITH THAT SENTIMENT
#THIS MEANS THE HIGHEST FREQUENCY WORDS IN EACH GROUP SHOULD BE GOOD PREDICTORS SPECIFICALLY FOR WHETHER A TWEET FALLS INTO EITHER GROUP, AND THE DICTIONARIES FOR EACH SENTIMENT CAN BE FULLY DISTINCT, WITH NO OVERLAP

#CREATING THE DICTIONARY: THE TOP 15 WORDS OF EACH SENTIMENT GROUP WILL BE UTILISED TO CREATE A DICTIONARY FOR CATEGORISING POSITIVE AND NEGATIVE SENTIMENT
positive.15.dictionary <- head(sentiment.dictionary.2,15)
positive.15.dictionary
negative.15.dictionary <- tail(sentiment.dictionary.2,15)
negative.15.dictionary
#THIS LIST OF WORDS IS DENOTED IN  (TABLE C2) IN THE MAIN REPORT

neg.words <- c("cancelled","now","hours","hold","delayed","still","call","hour","bag","late","need","phone","waiting","please","trying")
pos.words <- c("thanks","thank","great","love","awesome","much","best","good","guys","amazing","got","today","appreciate","crew","made")
mydict <- dictionary(list(negative = neg.words, positive = pos.words))
```
  These tables show:

*Chosen Dictionary Words: Top 15 Most Frequent (TF-IDF weighted) Words utilised uniquely in Positive Sentiment and Negative Sentiment Groups.*

**Now I actually use the dictionary classifier I have made to classify all the tweets in the sample as positive or negative in sentiment...** 
```{r , results=FALSE, message=FALSE, warning=FALSE}
#CREATING A WEIGHTED DOCUMENT-TERM MATRIX WHICH ADJUSTS FOR LENGTH OF TWEET AND USES MY OWN DICTIONARY, DEFINED IN SECTION H.0
tweets.dictionary <- dfm_weight(dfm(speechCorpus,remove_numbers=T,remove = stopwords("en"),remove_punct=T,dictionary = mydict),scheme="prop")
tweets.dictionary <- convert(tweets.dictionary,to="data.frame")

#I NOW CLASSIFY TWEETS WITH MORE NEGATIVE THAN POSITIVE WORDS AS NEGATIVE, AND ANY TWEETS WHICH DO NOT MEET THIS CONDITION AS POSITIVE
tweets.dictionary$score <- ifelse((tweets.dictionary$negative - tweets.dictionary$positive)>0,"1","0")

#THIS RESULTS IN A DATAFRAME, tweets.dictionary, WHICH CLASSIFIES EVERY TWEET AS EITHER POSITIVE OR NEGATIVE IN SENTIMENT, BASED ON MY DICTIONARY CLASSIFICATION WORDS
#FOR FULL PERFORMANCE ASSESSMENT OF THIS DICTIONARY-BASED SENTIMENT CLASSIFICATION APPROACH, SEE SECTION J.0
```

The resulting object, tweets.dictionary, has a score of 1 if the Tweet is wholly positive, and a score of 0 if it is wholly negative, and an intermediate score depending on the weighted frequency of positive and negative words it contains. 

Tweets are classified as negative only if they contained more negative than positive Dictionary words, meaning even tweets with no Dictionary words are classified as “positive”. This in-built positive sentiment bias prioritises Sensitivity. However, in the context of customer service this seems counter-intuitive, as addressing negative tweets holds greater urgency than addressing positive tweets which rarely require urgent engagement or remediation.

## The Supervised Machine Learning Approach

I apply LASSO, a Supervised Machine-Learning (Supervised) approach, to classify tweets into positive and negative sentiment by performing optimal word selection and regularisation, enhancing classification accuracy. This is achieved through classifying sentiment using 497 context-specific words derived from learning sentiment patterns in 2133 unique words. The tabel below allows comparison of the Top 15 most frequent words in each sentiment group generated by the Supervised approach, alongside those used to define the Dictionary.

**Here I demonstrate an alternative approach to dictionary formation and tweet-sentiment classification, which replaces the manual approach above...**
```{r , results=FALSE, message=FALSE, warning=FALSE}
#IN ORDER TO APPLY A SUPERVISED MACHINE LEARNING CLASSIFICATION, I CREATE A NEW DOCUMENT-TERM MATRIX, WEIGHTED BY DOCUMENT LENGTH BUT WITHOUT APPLYING MY DICTIONARY CLASSIFICATION. I ALSO REMOVE WORDS WHICH ARE TWEET-SPECIFIC, WHICH APPEAR ONLY IN 5 OR FEWER TWEETS
tweets.supervised <- dfm_weight(dfm(speechCorpus,remove_numbers=T,remove = stopwords("en"),remove_punct=T,),scheme="prop")
tweets.supervised <- tweets.supervised[,docfreq(tweets.supervised)>5]
tweets.supervised <- as.matrix(cbind(tweets$sentiment, tweets.supervised))
dim(tweets.supervised)

#THERE ARE 2132 UNIQUE WORDS CONTAINED IN 11,541 DISTINCT TWEETS

set.seed(1)
cv.rows <- sample(nrow(tweets.supervised),(nrow(tweets.supervised)/2))
cv.data <- tweets.supervised[cv.rows,]
test.data <- tweets.supervised[-cv.rows,]

lasso.tweets <- cv.glmnet(x=cv.data[,2:2133],y=cv.data[,1],family="binomial",type.measure="class")
tweets.preds <- predict(lasso.tweets, test.data[,2:2133], type="class")

#THE 15 WORDS WHICH MOST PREDICT POSITIVE AND NEGATIVE SENTIMENT IN TWEETS ARE: 

lasso.coef <- as.matrix(coef(lasso.tweets)[coef(lasso.tweets)[,1]!=0,])
lasso.coef <- as.matrix(lasso.coef[order(lasso.coef[,1]),])

#THIS ALLOWS ME TO IDENTIFY THE NUMBER OF WORDS USED FOR CLASSIFICATION IN THE OPTIMISED LASSO
#IN THIS CASE, JUST 497 OF THE ORIGINAL 2133 WORDS ARE USED FOR CLASSIFICATION
length(lasso.coef[lasso.coef!=0])

positive.15.supervised <- head(lasso.coef,15)
negative.15.supervised <- tail(lasso.coef,15)

positive.15.supervised
negative.15.supervised
```

**I now compare the top positive and negative sentiment words identified by the manual dictionary classifier and the machine learning classifier, to decide which is the better classifier for Airlines to use for future tweets...**

```{r warning=FALSE}
#PLOTTING THE TOP 15 POSITIVE AND NEGATIVE WORDS IN THE TWEETS, WHICH WERE APPLIED TO THE DICTIONARY, VS THOSE RESULTING FROM THE SUPERVISED CLASSIFICATION

#Dictionary Classifier: Positive Words"
positive.15.dictionary
#"Dictionary Classifier: Negative Words"
negative.15.dictionary
#"Supervised Classifier: Positive Words"
positive.15.supervised
#"Supervised Classifier: Negative Words"
negative.15.supervised
```

There is little overlap in words classified as strongly associated with each sentiment by the Dictionary and Supervised approach: only 3 positive and 2 negative terms are shared. The difference in words most associated with each sentiment reflects how LASSO does not simply select the highest frequency words in each group, but adjusts coefficients to inductively minimise classification error and penalise complexity, comparing models accounting for all sample words. Thus, this is likely to outperform the less exhaustive frequency-based Dictionary.

## Which Model is Best? Sensitivity, Specificity and Contextual Decisions...

**I finally evaluate the performance of the dictionary classifier and the supervised machine learning classifier relative to the actual manual classification of the tweets, to evaluate the sensitivity and specificity of each classification approach and select which is most suitable...**

*The Performance of the Dictionary Classifier*
```{r  , results=FALSE, message=FALSE, warning=FALSE}

#I ADD THE DICTIONARY-CLASSIFIED TWEETS AS A COLUMN OF THE tweets DATAFRAME
tweets$dictionary <- tweets.dictionary$score
tweets$dictionary
table(tweets$dictionary, tweets$sentiment)

#NOTE THAT (1) DENOTES NEGATIVE SENTIMENT. THIS MEANS THAT THE NUMBER OF "TRUE POSITIVE" CLASSIFIED TWEETS IS ACTUALLY IDENTIFIED BY (0,0), NOT (1,1) AS IS CONVENTIONALLY THE CASE. THROUGHOUT THIS ANALYSIS, THE TRUE POSITIVE RATE INDICATES THE NUMBER OF ACTUAL POSITIVE SENTIMENT TWEETS IDENTIFIED, DIVIDED BY THE TOTAL NUMBER OF POSITIVE TWEETS IN THE DATA.

#TRUE POSITIVE RATE:                  THERE IS 94.3% ACCURACY IN PREDICTING TWEETS WHICH ARE POSITIVE
2228/(2228+135)

#TEST ERROR RATE:                     TER = 45.99%
(135+5173)/(135+5173+4005+2228)

#TRUE NEGATIVE RATE:                  THERE IS 43.6% ACCURACY IN PREDICTING TWEETS WHICH ARE NEGATIVE
4005/(4005+5173)

#FALSE POSITIVE RATE: 
5173/(5173+4005)

```

*The Performance of the Lasso-Based Classification*
```{r  , results=FALSE, message=FALSE, warning=FALSE}
table(tweets.preds,test.data[,1])

#NOTE THAT (1) DENOTES NEGATIVE SENTIMENT. THIS MEANS THAT THE NUMBER OF "TRUE POSITIVE" CLASSIFIED TWEETS IS ACTUALLY IDENTIFIED BY (0,0), NOT (1,1) AS IS CONVENTIONALLY THE CASE. THROUGHOUT THIS ANALYSIS, THE TRUE POSITIVE RATE INDICATES THE NUMBER OF ACTUAL POSITIVE SENTIMENT TWEETS IDENTIFIED, DIVIDED BY THE TOTAL NUMBER OF POSITIVE TWEETS IN THE DATA.

#TRUE POSITIVE RATE:                THERE IS 61.5% ACCURACY IN PREDICTING TWEETS WHICH ARE POSITIVE
723/(723+452)

#TEST ERROR RATE:                   TER = 9.96%
(123+452)/(123+452+723+4473)

#TRUE NEGATIVE RATE:                THERE IS 97.3% ACCURACY IN PREDICTING TWEETS WHICH ARE NEGATIVE
4473/(4473+123)

#FALSE POSITIVE RATE: 
452/(723+452)

#THESE FIGURES ARE QUOTED IN (TABLE C5) IN THE MAIN REPORT
#THIS REVEALS A SIGNIFICANT IMPROVEMENT IN CLASSIFICATION PERFORMANCE, PARTICULARLY CENTRED ON THE TRUE NEGATIVE RATE (SPECIFICITY), WHICH IS PRIORITISED IN THIS REPORT AS IT IS MOST IMPORTANT FOR AIRLINES TO ADDRESS COMPLAINTS AND CONCERNS QUICKLY, RATHER THAN CLASSIFY PRAISE WHICH DOES NOT NEED TO BE RAPIDLY ADDRESSED OR REMEDIATED.

```


Test Error Rate (TER) is used to assess classifier performance and Specificity is prioritised over Sensitivity, as the cost of falsely identifying a tweet as negative and addressing it with urgency is far lower than that of misclassifying negative tweets and failing to provide timely customer service to problematic claims. A classifier with high Specificity is likely to enable airlines to respond to the most needy customers in real-time, eliminating ‘noise’ from positive tweets unlikely to require remediation. The chunks above affirm that based on the TER, Sensitivity and Specificity, the Supervised classifier is better, despite high Sensitivity of the Dictionary approach.

## Occam's Razor

Prioritising Specificity, the Supervised classifier improves detection of negative sentiment by 53.7%, misclassifying less than 3.00% of negative tweets, and generates a 46.4% lower TER. Not only is the Supervised classifier generally less prone to error, but it is particularly likely to increase effectiveness of customer response to negative tweets, despite increased risk of inefficiency-inducing false negatives as a result of its low 61.5% Sensitivity. This contrasts with the Dictionary approach, which misclassifies just 5.70% of positive tweets, but classifies negative tweets with even lower accuracy than the Supervised classifier does positive tweets. Hence, though both classifiers perform well, they do so at entirely different tasks. The Dictionary is most appropriate for airlines who prioritise positive customer engagement over problem or complaint remediation, whilst the Supervised approach is suited for airlines attempting to maximise efficiency of engagement with negative sentiment, to achieve faster remediation. Regardless, the low TER of the Supervised approach, of just 9.96%, indicates that it outperforms the Dictionary classifier overall, and particularly in negative sentiment classification, arguably more important than Sensitivity. Applying this Supervised classifier would therefore allow airlines to identify and address negative tweets in realtime, improving targeted responsiveness towards customers: a priority for airlines such as American, who’s most associated tweets involve negative words centred on resolvable problems including cancellations and delays.
