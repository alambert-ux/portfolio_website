---
date: "2017-10-31T21:28:43-05:00" 
description: "What Clusters of BBC iPlayer Users Exist and How Can We Recommend them Suitable Films?" 
draft: false
image: pic04.jpg  
keywords: ""
slug: scorsese
title: Entertainment  

---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="what-clusters-of-users-exist-and-how-can-we-recommend-them-suitable-films" class="section level1">
<h1><strong>What Clusters of Users Exist and How Can We Recommend them Suitable Films?</strong></h1>
<div id="applying-unsupervised-machine-learning-approaches-to-segment-users-and-recommend-movies" class="section level2">
<h2><strong>Applying Unsupervised Machine Learning Approaches to Segment Users and Recommend Movies</strong></h2>
<hr />
</div>
</div>
<div id="executive-summary" class="section level1">
<h1>Executive Summary</h1>
<p>The full reports for this project can be viewed as PDFs. See <a href="https://drive.google.com/file/d/1zxEOp_or8kaDXiYRs5ARM7F1FrVlhBeb/view?usp=sharing" target="_blank" title="Applying Cluster Analysis to Segment BBC iPlayer Users">Report 1</a> for analysis of targetable clusters of BBC iPlayer users, and <a href="https://drive.google.com/file/d/1Xw5V0KCXi2RPeYuUyy7tVp-KKxfpIFZS/view?usp=sharing" target="_blank" title="Building and Evaluating Movie Recommendation Systems">Report 2</a> for a step-by-step summary of the recommendation system construction and optimisation process.</p>
</div>
<div id="the-problem" class="section level1">
<h1>The Problem</h1>
<p>Intense competition for our finite attention has created a booming industry driven by ‘engagement’, seeking to hold our attention for long enough to monetize it through advertising or increased retention, and thus increased lifetime value to the platform.</p>
<p>Machine learning lies at the core of methodologies designed to identify, segment and target these users, keeping their eyes on the screen for as long as possible.</p>
<p><strong>The problem is therefore twofold:</strong></p>
<p><strong>Firstly:</strong> How can we identify which types of users exist on the platform, and target advertisement to their individual character profiles to <em>maximise ad revenue</em>?"</p>
<p><strong>Secondly:</strong> How can we ensure our content recommendations are of a high quality in order to <em>maximise customer engagement, and thus customer lifetime value</em> to the platform?</p>
<p><strong>To address each, I:</strong></p>
<p><strong>I)</strong> employ Cluster Analysis, an <em>unsupervised learning approach</em>, on a rich BBC iPlayer dataset to identify user profiles for targeted product and service advertising (under the assumption that this <em>is</em> a platform which advertises to its users)</p>
<p><strong>II)</strong> subsequently employ a multitude of <em>unsupervised</em> Collaborative Filtering (CF) approaches trained on IMDB movie rating data, in order to build an effective movie recommendation system for integration into the BBC iPlayer platform</p>
</div>
<div id="the-data" class="section level1">
<h1>The Data</h1>
<p>This investigation applies three key datasets: the <strong>first</strong> is of almost 94,000 BBC iPlayer users, across almost 20 characteristics, including genres watched, total viewing duration, average proportion of programs viewed, and most common viewing time; the <strong>second</strong> is a dataset of approximately 9,750 movies rated on IMDB, classified by genre; and the <strong>third</strong> is a dataset of 101,000 individual ratings given by IMDB users to these 9750 movies. While the first dataset is sufficient to identify clusters of BBC iPlayer users with an unsupervised learning approach, a combination of the second and third databases is necessary to build a ratings matrix suitable for construction of a movie recommendation system.</p>
<p>Note that IMDB data is used instead of BBC iPlayer movie data because iPlayer does not allow rating of its movies. Since the Recommendation System must be trained on a large ratings matrix, the world’s largest movie ratings database is the logical source. Of course, this also means that the Recommendation System will not be directly applicable to the BBC case, and a proxy ratings system will be required unless the BBC chooses to deploy a ratings feature in their iPlayer application. A reasonable proxy rating would for example be established via a composite measure considering: whether a certain type of user watched the movie at their “prime time”; whether the movie was completed; whether the movie was rewatched; whether it was reshared; how often it was paused, i.e. whether it was watched in a single sitting, and so on.</p>
</div>
<div id="the-solution" class="section level1">
<h1>The Solution</h1>
<p>The following analyses first (I) specify and evaluate key clusters of BBC iPlayer users to be targeted with tailored advertisement, and subsequently (II) build an optimal item-based collaborative filtering movie recommendation system (with matrix factorisation) to boost engagement and therefore customer lifetime value (CLV).</p>
<div id="iii.-what-clusters-of-bbc-iplayer-users-exist" class="section level2">
<h2>I/II. What Clusters of BBC iPlayer Users Exist?</h2>
<p>I focus specifically on the three clusters generated by the full-sample k-means clustering approach with k=3, since this seems most consistent with my selection criteria. Note however that both the PAM and Hierarchical (Ward) approaches ratify the substantive existence of these clusters.</p>
<p><strong>The “Family”, Cluster 2</strong></p>
<p>Both the k-means and hierarchical clustering (Ward method) methodologies (Figures 7 to 9), identified a cluster of “Family” BBC iPlayer viewers with a strong preference for Children’s and Learning content and relatively high engagement during the Daytime, and particularly in the Afternoon. This is consistent with a conception of accounts used mainly when children return home from school and either watch children’s shows or learning programs. Given the stability of this cluster’s centers and principal components between the most effective clustering methodologies and across the sample and both subsamples, this profile is the most robust (and thereby plausible) cluster.</p>
<p><strong>The “Working Professional”, Cluster 1</strong></p>
<p>Though with less certainty, since this cluster is less stable across subsamples and clustering methodologies, I identify a cluster of viewers with relatively high engagement who have a propensity towards Drama, Comedy and Entertainment shows in the Evening and Nighttime. This seems consistent with adult viewers who work during the day and use BBC iPlayer to de-stress with films and series after work. Though not particularly pronounced in the k-means clusters, this cluster is clearly identifiable as Cluster 2 in the PAM methodology, which implies it is somewhat plausible.</p>
<p><strong>The “Daytime Viewer”, Cluster 3</strong></p>
<p>Finally, I distinguish a cluster of viewers that tend to watch BBC iPlayer during the day, and largely watch News, Factual, Sport and Weather genres with low engagement. This cluster is consistent with profiles of elderly and unemployed viewers who are at home during the day but like to stay informed. Although this cluster does not stand out particularly in the k-means method, it is pronounced in the cluster center plots of both the PAM (where it is represented by Cluster 1) and Hierarchical (Ward) method (represented by Cluster 2), suggesting that it too is a plausible segment.</p>
</div>
<div id="iiii.-how-can-we-recommend-them-suitable-films" class="section level2">
<h2>II/II. How Can We Recommend Them Suitable Films?</h2>
<p>Of the three recommendation model specifications evaluated, LIBMF minimises RMSE regardless of the prevalence of ratings associated with the users and movies it is trained on. This model-based matrix factorisation collaborative filtering approach consists of decomposing the sparse user-item interaction matrix into a product of two smaller, more densely populated matrices representing latent variable representations for users and movies, respectively. Resultingly, close users in terms of movie preferences, as well as close movies in terms of characteristics, are assigned close representations in the latent space. Specifically, of the 15 models surveyed, I recommend use of LIBMF trained on ratings from users and movies which have given or received at least 50 ratings, since this is when RMSE is minimised. Under the assumption that there are no more than 10 latent variables (factors, such as genre) describing movie ‘types’, which can be used to describe user preferences, I set dimensions to 10.</p>
</div>
</div>
<div id="analysis-in-r" class="section level1">
<h1><strong>Analysis in R</strong></h1>
<hr />
</div>
<div id="iii.-what-clusters-of-bbc-iplayer-users-exist-1" class="section level1">
<h1><strong>I/II. What Clusters of BBC iPlayer Users Exist?</strong></h1>
<p>It is particularly important to know which customer segments are interested in what content and how this drives their engagement with the BBC (often measured by how happy they are to pay for the TV licensing fees).</p>
</div>
<div id="cleaned-data" class="section level1">
<h1>Cleaned Data</h1>
<p>As the original view data is already processed and cleaned, I now generate a user-based database which I will use to train clustering algorithms to identify meaningful clusters in the data.</p>
<p>Let’s load the cleaned data and investigate what’s in the data. See below for column descriptions.</p>
<pre class="r"><code>cleaned_BBC_Data &lt;- read_csv(here::here(&#39;data&#39;,&#39;Results_Step1.csv&#39;))

glimpse(cleaned_BBC_Data) </code></pre>
<pre><code>## Rows: 313,256
## Columns: 17
## $ user_id                   &lt;chr&gt; &quot;cd2006&quot;, &quot;cd2006&quot;, &quot;cd2006&quot;, &quot;cd2006&quot;, &quot;cd…
## $ program_id                &lt;chr&gt; &quot;b8fbf2&quot;, &quot;e2f113&quot;, &quot;0e0916&quot;, &quot;ca03b9&quot;, &quot;cf…
## $ series_id                 &lt;chr&gt; &quot;e0480e&quot;, &quot;933a1b&quot;, &quot;b68e79&quot;, &quot;5d0813&quot;, &quot;eb…
## $ genre                     &lt;chr&gt; &quot;Comedy&quot;, &quot;Factual&quot;, &quot;Entertainment&quot;, &quot;Spor…
## $ start_date_time           &lt;dttm&gt; 2017-01-19 22:17:04, 2017-02-14 19:12:36, …
## $ streaming_id              &lt;chr&gt; &quot;1484864257965_1&quot;, &quot;1487099603980_1&quot;, &quot;1484…
## $ prog_duration_min         &lt;dbl&gt; 1.850000, 0.500000, 1.366667, 1.616667, 8.5…
## $ time_viewed_min           &lt;dbl&gt; 1.85000000, 0.49908333, 1.36666667, 1.61666…
## $ duration_more_30s         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ time_viewed_more_5s       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ percentage_program_viewed &lt;dbl&gt; 1.000000000, 0.998166667, 1.000000000, 1.00…
## $ watched_more_60_percent   &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0…
## $ month                     &lt;dbl&gt; 1, 2, 1, 2, 3, 2, 4, 3, 4, 3, 3, 4, 3, 3, 2…
## $ day                       &lt;dbl&gt; 5, 3, 4, 1, 1, 3, 1, 6, 7, 7, 1, 5, 7, 3, 2…
## $ hour                      &lt;dbl&gt; 22, 19, 21, 14, 20, 19, 20, 21, 21, 20, 18,…
## $ weekend                   &lt;chr&gt; &quot;weekday&quot;, &quot;weekday&quot;, &quot;weekday&quot;, &quot;weekend&quot;,…
## $ time_of_day               &lt;chr&gt; &quot;Evening&quot;, &quot;Evening&quot;, &quot;Evening&quot;, &quot;Day&quot;, &quot;Ev…</code></pre>
<p>The column descriptions are as follows.</p>
<ol style="list-style-type: lower-alpha">
<li><p>user_id – a unique identifier for the viewer</p></li>
<li><p>program_id and series_id – these identify the program and the series that the program belongs to</p></li>
<li><p>genre – the programme’s genre (e.g., drama, factual, news, sport, comedy, etc)</p></li>
<li><p>start_date_time – the streaming start date/time of the event</p></li>
<li><p>Streaming id – a unique identifier per streaming event</p></li>
<li><p>prog_duration_min – the program duration in minutes</p></li>
<li><p>time_viewed_min – how long the customer watched the program in minutes</p></li>
<li><p>duration_more_30s - equals 1 if the program duration is more than 30 seconds, equals 0 otherwise</p></li>
<li><p>time_viewed_more_5s - equals 1 if time_viewed is more than 5 seconds, equals 0 otherwise</p></li>
<li><p>percentage_program_viewed – percantage of the program viewed</p></li>
<li><p>watched_more_60_percent – equals 1 if more than 60% of the program is watched, equals 0 otherwise</p></li>
<li><p>month, day, hour, weekend – timing of the viewing</p></li>
<li><p>time_of_day – equals “Night” if the viewing occurs between 22 and 6am, “Day” if it occurs between 6AM and 14, “Afternoon” if the it occurs between 14 and 17, “Evening” otherwise</p></li>
</ol>
<p>Before I proceed let’s consider the usage in January only.</p>
<pre class="r"><code>cleaned_BBC_Data&lt;-filter(cleaned_BBC_Data,month==1)</code></pre>
</div>
<div id="user-based-data" class="section level1">
<h1>User based data</h1>
<p>I will try to create meaningful customer segments that describe users of the BBC iPlayer service. First I need to change the data to user based and generate a summary of their usage.</p>
<div id="data-format" class="section level2">
<h2>Data format</h2>
<p>The data is presented to us in an event-based format (every row captures a viewing event). However I need to detect the differences between the general watching habits of users. To do so, I need to come up with variables to capture these differences.</p>
</div>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>First, let’s generate the following variables for each user.</p>
<ol style="list-style-type: lower-roman">
<li>Total number of shows watched and ii. Total time spent watching shows on iPlayer by each user in the data</li>
</ol>
<pre class="r"><code>userData&lt;-cleaned_BBC_Data %&gt;% group_by(user_id) %&gt;% summarise(noShows=n(), total_Time=sum(time_viewed_min)) </code></pre>
<ol start="3" style="list-style-type: lower-roman">
<li>Proportion of shows watched during the weekend for each user.</li>
</ol>
<pre class="r"><code>#Let&#39;s find the number of shows on weekend and weekdays
userData2&lt;-cleaned_BBC_Data %&gt;% group_by(user_id,weekend) %&gt;% summarise(noShows=n())

#Let&#39;s find percentage in weekend and weekday
userData3 = userData2%&gt;% group_by(user_id) %&gt;% mutate(weight_pct = noShows / sum(noShows))

#Let&#39;s create a data frame with each user in a row.
userData3&lt;-select (userData3,-noShows)
userData3&lt;-userData3%&gt;% spread(weekend,weight_pct,fill=0) %&gt;%as.data.frame()
#Let&#39;s merge the final result with the data frame from the previous step.
userdatall&lt;-left_join(userData,userData3,by=&quot;user_id&quot;)</code></pre>
<ol start="4" style="list-style-type: lower-roman">
<li>Proportion of shows watched during different times of day for each user.</li>
</ol>
<pre class="r"><code>#Code in this block follows the same steps above.
userData2&lt;-cleaned_BBC_Data %&gt;% group_by(user_id,time_of_day) %&gt;% summarise(noShows=n()) %&gt;% mutate(weight_pct = noShows / sum(noShows))

userData4&lt;-select (userData2,-c(noShows))
userData4&lt;-spread(userData4,time_of_day,weight_pct,fill=0)

userdatall&lt;-left_join(userdatall,userData4,by=&quot;user_id&quot;)</code></pre>
<blockquote>
<p>The proportion of shows watched in each genre by each user.</p>
</blockquote>
<pre class="r"><code>userData5 &lt;- cleaned_BBC_Data %&gt;% 
  group_by(user_id, genre) %&gt;% 
  summarise(count=n()) %&gt;% 
  mutate(total= sum(count), genre_proportion=((count/total))) %&gt;% 
  select(-c(3,4)) %&gt;% 
  pivot_wider(names_from=&quot;genre&quot;,values_from=&quot;genre_proportion&quot;) # could use spread alternatively

userdatall&lt;-left_join(userdatall,userData5,by=&quot;user_id&quot;)</code></pre>
<blockquote>
<p>One more variable: Average percentage program viewed for each user</p>
</blockquote>
<pre class="r"><code>#DIFFERENTIATING BETWEEN &quot;HIGH ENGAGEMENT&quot; AND &quot;LOW ENGAGEMENT&quot; USERS

#option 1

userData6 &lt;- cleaned_BBC_Data %&gt;% 
  group_by(user_id) %&gt;% 
  summarise(mean_percentage_viewed=mean(percentage_program_viewed))
  
userdatall&lt;-left_join(userdatall,userData6,by=&quot;user_id&quot;)</code></pre>
<p>The average percentage program viewed is useful as a proxy for user engagement. Specifically, I seek to differentiate between users who are served content which they engage with (i.e. watch the majority of), and those who are less likely to engage with the content they watch (watch consistently low proportions of each show on average).</p>
</div>
</div>
<div id="visualizing-user-based-data" class="section level1">
<h1>Visualizing user-based data</h1>
<p>Next I visualize the information captured in the user based data. Let’s start with the correlations.</p>
<pre class="r"><code>library(&quot;GGally&quot;)
userdatall %&gt;% 
  select(-user_id) %&gt;% #keep Y variable last
  ggcorr(method = c(&quot;pairwise&quot;, &quot;pearson&quot;), layout.exp = 3,label_round=2, label = TRUE,label_size = 2,hjust = 1)+
  labs(subtitle=&quot;There is a strong correlation between some variables such as learning &amp; no genre, weekend &amp; weekday&quot;, title= &quot;Correlation Table&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/correlations-1.png" width="960" /></p>
<p>Most correlated variables are:</p>
<ul>
<li>Learning and NoGenre (cor = 1)</li>
<li>Weekend and weekday (cor = -1)</li>
</ul>
<p>In addition, the following variables worth highlighting:
- RelEthics and NoGenre (cor = 0.97)
- noShows and total_Time (cor = 0.92)</p>
<p>The implication for this is as follows: These correlated variables will generate predictable patterns in cluster centres (for instance, means). This means there is some redundancy in the data, since by identifying a cluster centre for one variable of the correlated pair, I are able to predict the cluster centre for the other. Thus, I could reduce the number of dimensions without losing information by dropping one of the correlated variables, which would result in more meaningful clusters, however this input may result in some distance being artificially changed, thus would require careful post-evaluation.</p>
<blockquote>
<p>Distribution of noShows and total_Time visualized with box-whisker plots and histograms.</p>
</blockquote>
<pre class="r"><code>userdatall_plots &lt;- userdatall %&gt;% 
  mutate(mean_noShows=mean(noShows), median_noShows=median(noShows),mean_time=mean(total_Time), median_time=median(total_Time))

p1 &lt;- ggplot(userdatall_plots, aes(x=as.numeric(total_Time)))+
  geom_boxplot(alpha=0.1)+
  scale_x_continuous(limits = c(0,300))+ # I do want to see where first outliers appear
  labs(x =&quot;Total Time (minutes)&quot;, title= &quot;Distribution of Total Time Watched&quot;) + theme_fivethirtyeight() + theme(axis.title=element_text(), axis.text.y = element_blank(),
         axis.ticks.y = element_blank())

p2 &lt;- ggplot(userdatall_plots, aes(x=as.numeric(noShows)))+
   geom_boxplot(alpha=0.1)+
  scale_x_continuous(limits = c(0,50))+ # I do want to see where first outliers appear
  labs(x =&quot;Number of Shows&quot;, title= &quot;Distribution of Number of Shows&quot;)+
  theme_fivethirtyeight() + theme(axis.title=element_text(), axis.text.y = element_blank(),
         axis.ticks.y = element_blank())

p3 &lt;- ggplot(userdatall_plots, aes(x=as.numeric(total_Time)))+
  geom_histogram()+
  scale_x_log10() +
  labs(x =&quot;Total Time (minutes)&quot;,y=&quot;Count&quot;) + theme_fivethirtyeight() + theme(axis.title=element_text()) +
    geom_vline(xintercept=userdatall_plots$mean_time, size=0.5, color=&quot;red&quot;) + annotate(geom = &#39;text&#39;, colour= &quot;white&quot;, label = &quot;Sample Mean&quot;, x = userdatall_plots$mean_time+100, y = 450, angle=270) + geom_vline(xintercept=userdatall_plots$median_time, size=0.5, color=&quot;blue&quot;) + annotate(geom = &#39;text&#39;, colour= &quot;white&quot;, label = &quot;Sample Median&quot;, x = userdatall_plots$median_time+30, y = 420, angle=270)


p4 &lt;- ggplot(userdatall, aes(x=as.numeric(noShows)))+
  geom_histogram()+
  scale_x_log10() +
  labs(x = &quot;Number of Shows&quot;,y=&quot;Count&quot;) +
   theme_fivethirtyeight() + theme(axis.title=element_text()) + 
  geom_vline(xintercept=userdatall_plots$mean_noShows, size=0.5, color=&quot;red&quot;) + annotate(geom = &#39;text&#39;, label = &quot;Sample Mean&quot;, x = userdatall_plots$mean_noShows+3, y = 1800, angle=270) + geom_vline(xintercept=userdatall_plots$median_noShows, size=0.5, color=&quot;blue&quot;) + annotate(geom = &#39;text&#39;, label = &quot;Sample Median&quot;, x = userdatall_plots$median_noShows+1, y = 1700, angle=270)

(p1 + p3) /( p2 + p4)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/box_whisker_histogram-1.png" width="960" /></p>
<p>Number of shows and total time watched are both heavily positively skewed (median&gt;mean): the former has a mode of only one show watched, whereas most people spend around 100 minutes watching shows on BBC iPlayer in the researched period. This strong skew reflects the influence of outliers at the upper limit of both variables, which are observed in the box-and-whisker plots.</p>
<p>These outliers can have significant impact on the structure of clusters, and in order to reduce this impact, I will employ a log transformation.</p>
<div id="delete-infrequent-users" class="section level2">
<h2>Delete infrequent users</h2>
<p>I delete the records for users whose total view time is less than 5 minutes and who views 5 or fewer programs. These users are not very likely to be informative for clustering purposes. Or I can view these users as a ``low-engagement’’ cluster.</p>
<pre class="r"><code>userdata_red&lt;-userdatall%&gt;%filter(total_Time&gt;=5)%&gt;%filter(noShows&gt;=5)
ggplot(userdata_red, aes(x=total_Time)) +geom_histogram(binwidth=25)+labs(x=&quot;Total Time Watched (mins)&quot;, y= &quot;Count&quot;, title=&quot;Distribution of Total Time Watched Without Low-Engagement Users&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/delete-1.png" width="960" /></p>
<pre class="r"><code>glimpse(userdata_red)</code></pre>
<pre><code>## Rows: 3,625
## Columns: 22
## $ user_id                &lt;chr&gt; &quot;002b2e&quot;, &quot;0059d9&quot;, &quot;00aad3&quot;, &quot;00c6e6&quot;, &quot;00caa…
## $ noShows                &lt;int&gt; 31, 20, 8, 16, 16, 5, 6, 7, 5, 7, 36, 5, 8, 5,…
## $ total_Time             &lt;dbl&gt; 534.355573, 446.054906, 38.004417, 148.464367,…
## $ weekday                &lt;dbl&gt; 0.8709677, 0.7500000, 1.0000000, 0.8125000, 0.…
## $ weekend                &lt;dbl&gt; 0.12903226, 0.25000000, 0.00000000, 0.18750000…
## $ Afternoon              &lt;dbl&gt; 0.00000000, 0.15000000, 0.00000000, 0.00000000…
## $ Day                    &lt;dbl&gt; 0.09677419, 0.45000000, 0.00000000, 0.00000000…
## $ Evening                &lt;dbl&gt; 0.8064516, 0.4000000, 0.7500000, 1.0000000, 0.…
## $ Night                  &lt;dbl&gt; 0.09677419, 0.00000000, 0.25000000, 0.00000000…
## $ News                   &lt;dbl&gt; 0.03225806, 0.50000000, 0.37500000, NA, NA, NA…
## $ Entertainment          &lt;dbl&gt; 0.06451613, 0.10000000, NA, 0.37500000, NA, NA…
## $ Comedy                 &lt;dbl&gt; 0.06451613, NA, 0.25000000, NA, NA, NA, 0.3333…
## $ Drama                  &lt;dbl&gt; 0.3225806, NA, NA, 0.0625000, 0.8125000, NA, 0…
## $ Factual                &lt;dbl&gt; 0.41935484, 0.30000000, 0.25000000, 0.50000000…
## $ Music                  &lt;dbl&gt; 0.06451613, NA, NA, NA, NA, NA, NA, NA, 0.2000…
## $ Sport                  &lt;dbl&gt; 0.03225806, 0.05000000, 0.12500000, NA, NA, NA…
## $ Weather                &lt;dbl&gt; NA, 0.05000000, NA, NA, NA, NA, NA, NA, NA, NA…
## $ NoGenre                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
## $ RelEthics              &lt;dbl&gt; NA, NA, NA, 0.06250000, NA, NA, NA, NA, NA, NA…
## $ Children               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
## $ Learning               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
## $ mean_percentage_viewed &lt;dbl&gt; 0.29720926, 0.45242522, 0.11615515, 0.23324406…</code></pre>
</div>
</div>
<div id="clustering-with-k-means" class="section level1">
<h1>Clustering with K-Means</h1>
<p>Now I are ready to find clusters in the BBC iPlayer viewers. I will start with the K-Means algorithm.</p>
<div id="training-a-k-means-model" class="section level2">
<h2>Training a K-Means Model</h2>
<p>To train a K-Means model, I start with 2 clusters and I de-select <code>user_id</code> variable. Also I scale the data. I use 50 random starts. I could use more starts, but 50 starting points should be enough to get a accurate result.</p>
<pre class="r"><code>set.seed(1) # set seed for reproducibility

k=2
n=50

# Get rid of variables that I might not need. Do not include no shows as well because it is highly correlated with total time

userdata_sel &lt;- userdata_red %&gt;%
  select(-c(user_id, weekday, noShows, NoGenre)) %&gt;%  # drop one of each of the highly correlated variables and userID
  mutate(total_Time = log1p(total_Time)) #applying a log transfomation in order to reduce the impact of outliers

#skim(userdata_sel) 

userdata_sel[is.na(userdata_sel)] = 0 #change NA to 0 (NAs appear only in genre percentages)

userdata_sel&lt;-data.frame(scale(userdata_sel)) #scale the data

#sanity check for scaling data

print(mean(userdata_sel$total_Time)) </code></pre>
<pre><code>## [1] 1.33796e-16</code></pre>
<pre class="r"><code>print(sd(userdata_sel$total_Time))</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>#train k-means clustering (and display cluster sizes via graph=TRUE)

model_kmeans_2clusters&lt;-eclust(userdata_sel, &quot;kmeans&quot;, k = 2,nstart = 50, graph = TRUE)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/fit_kmean_k2-1.png" width="672" /></p>
<pre class="r"><code>#check components
summary(model_kmeans_2clusters) </code></pre>
<pre><code>##              Length Class      Mode   
## cluster      3625   -none-     numeric
## centers        36   -none-     numeric
## totss           1   -none-     numeric
## withinss        2   -none-     numeric
## tot.withinss    1   -none-     numeric
## betweenss       1   -none-     numeric
## size            2   -none-     numeric
## iter            1   -none-     numeric
## ifault          1   -none-     numeric
## clust_plot      9   gg         list   
## silinfo         3   -none-     list   
## nbclust         1   -none-     numeric
## data           18   data.frame list</code></pre>
<pre class="r"><code>#obtain the size of the clusters
model_kmeans_2clusters$size</code></pre>
<pre><code>## [1] 2381 1244</code></pre>
<pre class="r"><code># add clusters info the data frame
userdata_sel_clusters&lt;-mutate(userdata_sel, cluster = as.factor(model_kmeans_2clusters$cluster))</code></pre>
<p>There are two clusters of following sizes: 2381 and 1244. There is no need to increase the number of starts, since cluster sizes do not change when I do so (i.e. cluster sizes are not sensitive to increases in the number of starts from 50 to 100, to 150).</p>
</div>
<div id="visualizing-the-results" class="section level2">
<h2>Visualizing the results</h2>
<div id="cluster-centers" class="section level3">
<h3>Cluster centers</h3>
<p>I plot the normalized cluster centers and describe the clusters that the algorithm suggests.</p>
<pre class="r"><code>#new data frame with cluster centers and cluster numbers
cluster_centers&lt;-data.frame(cluster=as.factor(c(1:2)),model_kmeans_2clusters$centers)

#transpose this data frame
cluster_centers_t&lt;-cluster_centers %&gt;% gather(variable,value,-cluster,factor_key = TRUE)

#plot the centers for k=2
graphkmeans_2clusters&lt;-ggplot(cluster_centers_t, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;Cluster 1: Spend More Time Watching BBC iPlayer and Watch More Drama and Comedy Shows, Particularly in the Evening and at Night with Higher Engagement \nCluster 2: Spend Less Time Watching BBC iPlayer, Have Broader Tastes and Watch Mainly News and Children&#39;s TV Genres in the Afternoon and Daytime with Lower Engagement&quot;, title=&quot;K-means Cluster Centers, k=2&quot;, x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

graphkmeans_2clusters</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/cluster_centers-1.png" width="1440" />
I generated two clusters, whose main differentiators are viewing habits and engagement (average percentage of content viewed), rather than the watched content.</p>
<p><strong>Cluster 1</strong>
Cluster 1 represents viewers that spend more time watching BBC iPlayer, are more engaged (on average watch a higher percentage of the content they view), and mainly watch shows during the evening, night and weekends. In addition, in terms of content, they tend to watch more drama, comedy and entertainment shows.</p>
<p>The viewer profile for Cluster 1 could be the adult viewer watching to relax during the evenings after work, and on weekends.</p>
<p><strong>Cluster 2</strong>
Cluster 2 is effectively the opposite of Cluster 1 in their watching behaviour. They they spend less time watching BBC iPlayer, are less engaged with the content (watch a lower percentage of the content they view, on average), and mostly watch during daylight (day, afternoon). In terms of content, they tend to watch predominantly cotent for children, learning, and News genres, and engage with a much broader spectrum of genres than the Cluster 1 viewer.</p>
<p>The viewer profile that comes to mind for Cluster 2 is the family household viewer, split between daytime children shows and news.
<strong>Leveraging Clusters to Improve the Viewer Experience</strong></p>
<p>These clusters are somewhat meaningful, however the limits of 2 clusters mean that I cannot distinguish more types of users interested in particular content/genres.</p>
<p>However, this does allow us to make speculative efforts to improve the viewer experience by suggesting a relationship between viewing habits and a few genres. In particular, I expect that recommending shorter shows such as news or children’s shows that do not require much attention from users might increase viewer experience during the day, and suggest more complex or longer shows (more demanding in terms of attention and time) during evening/night time.</p>
</div>
<div id="clusters-vs-variables" class="section level3">
<h3>Clusters vs variables</h3>
<p>I plot a scatter plot for the viewers with respect to total_Time and weekend variables with color set to the cluster number of the user.</p>
<pre class="r"><code>ggplot(userdata_sel_clusters, aes(x = total_Time, y = weekend, color =  as.factor(cluster))) +
  geom_jitter(alpha=0.8)+labs(color = &quot;Cluster&quot;, x=&quot;Total Time&quot;,y=&quot;Weekend&quot;, title=&quot;Total Time Watched has a Strong Impact on Determining Clusters of Viewers&quot;, subtitle=&quot;Heterogeneity Across Weekend Viewing and Total Viewing Time Variables Between Clusters&quot;) + theme_fivethirtyeight() + theme(axis.title=element_text())</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/distribution_wrt_variables-1.png" width="960" />
I do observe that most of points are located in the lower part of the graph, which suggests that the Weekend variable is not that crucial in determining clusters. Evidently, total_Time plays a more prominent role, since points are more dispersed along the x-axis, with Cluster 1 appearing to have a higher average level of total_Time than Cluster 2.</p>
</div>
<div id="clusters-vs-pca-components" class="section level3">
<h3>Clusters vs PCA components</h3>
<p>Repeat the previous step and use the first two principle components using <code>fviz_cluster</code> function.</p>
<pre class="r"><code>fviz_cluster(model_kmeans_2clusters, userdata_sel, palette = &quot;Set2&quot;, main=&quot;K-Means Cluster Plot Across Two Principal Components&quot;,subtitle=&quot;All Variables are Log Transformed to Mitigate the Effect of Outliers&quot;, x=&quot;Dimension 1 (10.9%)&quot;, y=&quot;Dimension 2 (9.6%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/cluster_centers2-1.png" width="1440" /></p>
<p>As the plot of cluster centres previously suggested, there is no clear distinction between clusters, since they overlap when visualised in terms of the first two principal components.</p>
</div>
<div id="clusters-vs-pca-components-without-log-transform" class="section level3">
<h3>Clusters vs PCA components without log transform</h3>
<p>As a robustness check, I use K-means method again but this time do not log transform <code>total time</code> and include <code>no_shows</code>.</p>
<pre class="r"><code>userdata_selv2 &lt;- userdata_red %&gt;%
  select(-c(user_id, weekday, NoGenre))  # drop one of high correlated variables and userID

#skim(userdata_sel) 

userdata_selv2[is.na(userdata_selv2)] = 0 #change NA to 0 (NAs appear only in genre percentages)

userdata_selv2&lt;-data.frame(scale(userdata_selv2)) #scale the data

set.seed(1) # set seed for reproducibility

#train kmeans clustering

model_kmeans_2clustersv2&lt;-eclust(userdata_selv2, &quot;kmeans&quot;, k = 2,nstart = 50, graph = TRUE)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/cluster_centers_without_log_transform-1.png" width="1440" /></p>
<pre class="r"><code>#Size of the clusters

model_kmeans_2clustersv2$size</code></pre>
<pre><code>## [1] 3443  182</code></pre>
<pre class="r"><code>#Plot centers for k=2

#new data frame with cluster centers and cluster numbers
cluster_centers2&lt;-data.frame(cluster=as.factor(c(1:2)),model_kmeans_2clustersv2$centers)

#transpose this data frame
cluster_centers2_t&lt;-cluster_centers2 %&gt;% gather(variable,value,-cluster,factor_key = TRUE)

#plot the centers
ggplot(cluster_centers2_t, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;Cluster 1: Spend More Time Watching BBC iPlayer and Watch More Drama and Comedy Shows, Particularly in the Evening and at Night with Higher Engagement \nCluster 2: Spend Less Time Watching BBC iPlayer, Have Broader Tastes and Watch Mainly News and Children&#39;s TV Genres in the Afternoon and Daytime with Lower Engagement&quot;, title=&quot;K-means Cluster Centers, k=2&quot;, x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Number of Shows&quot;, &quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/cluster_centers_without_log_transform-2.png" width="1440" /></p>
<pre class="r"><code>#PCA visualization
fviz_cluster(model_kmeans_2clustersv2, userdata_selv2, palette = &quot;Set2&quot;, main=&quot;K-Means Cluster Plot Across Two Principal Components&quot;,subtitle=&quot;Variables are Not Log Transformed&quot;, x=&quot;Dimension 1 (11.1%)&quot;, y=&quot;Dimension 2 (10.1%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/cluster_centers_without_log_transform-3.png" width="1440" /></p>
<p><strong>Cluster Sizes</strong></p>
<p>There is a significant difference in terms of cluster sizes between the non-log-transformed (3444, 182) and log-transformed (2381, 1244) partitions.</p>
<p><strong>Plotting Centers</strong></p>
<p>The clustering without log transformation is less meaningful than when variables are log transformed, since the Cluster 1 center has values approximately to zero for almost all variables. This is because of a number of outliers which bias each Cluster. Due to this bias, the k-means algorithm is unable to differentiate Cluster 1 sufficiently well from Cluster 2, as it is when the log transformation is applied.</p>
<p><strong>Principal Components</strong></p>
<p>The PCA plot emphasises the presence of outliers, which are accountable for an even greater overlap between clusters and lack of distinct variable traits in the Cluster 1 centre.</p>
</div>
</div>
<div id="elbow-chart" class="section level2">
<h2>Elbow Chart</h2>
<p>I use an elbow chart to identify a reasonable range for the number of clusters.</p>
<pre class="r"><code>fviz_nbclust(userdata_sel,kmeans,k.max = 20, method = &quot;wss&quot;) +
  labs(title=&quot;Optimal Number of Clusters&quot;, subtitle = &quot;Elbow Method&quot;, x=&quot;Number of Clusters, k&quot;, y=&quot;Total Within Sum of Squares&quot;) + theme_fivethirtyeight() + theme(axis.title = element_text()) + scale_y_continuous(breaks=seq(25000,65000,5000), limits=c(25000,65000)) + geom_vline(xintercept=13, size=0.5, color=&quot;red&quot;) + geom_vline(xintercept=17, size=0.5, color=&quot;red&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/elbow-1.png" width="960" /></p>
<p>From the plot above, I observe an elbow shape when k = 13, where the reduction in total within sum of squares becomes considerably smaller than when k = 12, suggesting that I need at least 13 clusters in order not to miss out on a considerable reduction in total within sum of squares. Further, starting from k = 17, the marginal reduction seems to be minor as k increases. Therefore, 13 to 17 could be a good range for the number of clusters in my case.</p>
</div>
<div id="silhouette-method" class="section level2">
<h2>Silhouette method</h2>
<p>As another robustness check, I also use the silhouette method to identify a reasonable range for the number of clusters.</p>
<pre class="r"><code>fviz_nbclust(userdata_sel, kmeans, method = &quot;silhouette&quot;,k.max = 20)+labs(title=&quot;Optimal Number of Clusters&quot;, subtitle = &quot;Silhouette Method&quot;, x=&quot;Number of Clusters, k&quot;, y=&quot;Average Silhouette Width&quot;) + theme_fivethirtyeight() + theme(axis.title = element_text()) + scale_y_continuous(breaks=seq(0,0.20,0.05), limits=c(0,0.20))</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/silhouette-1.png" width="960" />
The silhouette plot recommends k = 13 as the optimal number of clusters for my analysis. This is because, when k = 13, the average silhouette width is maximised, which means that, on average, observations are best matched to their own cluster and poorly matched to all other clusters.</p>
<blockquote>
<p>Optimal number of clusters</p>
</blockquote>
<p>From the elbow chart, I can identify an elbow shape when k = 13, and a relatively flat total within sum of squares from k = 13 - 17. Using the silhouette method, I observe that the maximum average silhouette width, and thus the maximum average distance between clusters, occurs when k = 13. Further, relatively high silhouette widths for k = 14 - 17 ratify my findings from the elbow chart that k = 13-17 is a plausible range for the optimal number of clusters under k-means clustering.</p>
</div>
<div id="comparing-k-means-with-different-k" class="section level2">
<h2>Comparing k-means with different k</h2>
<blockquote>
<p>Simplifying: focusing in only on clusters using k-means for k = 3, 4 and 5 to identify which is most plausible.</p>
</blockquote>
<pre class="r"><code>#Fit k-means models

set.seed(1)
model_kmeans_3clusters&lt;-eclust(userdata_sel, &quot;kmeans&quot;, k = 3, nstart = 50, graph = FALSE)
model_kmeans_4clusters&lt;-eclust(userdata_sel, &quot;kmeans&quot;, k = 4, nstart = 50, graph = FALSE)
model_kmeans_5clusters&lt;-eclust(userdata_sel, &quot;kmeans&quot;, k = 5, nstart = 50, graph = FALSE)

# size of clusters
model_kmeans_3clusters$size</code></pre>
<pre><code>## [1] 1841  181 1603</code></pre>
<pre class="r"><code>model_kmeans_4clusters$size</code></pre>
<pre><code>## [1]  180 1214  622 1609</code></pre>
<pre class="r"><code>model_kmeans_5clusters$size</code></pre>
<pre><code>## [1]  182  706  813 1075  849</code></pre>
<pre class="r"><code>#PCA visualizations

#FOR K=3
kmeans_3_pca &lt;- fviz_cluster(model_kmeans_3clusters, userdata_sel, palette = &quot;Set2&quot;, main=&quot;K-Means Cluster Plot Across Two Principal Components, k = 3&quot;, x=&quot;Dimension 1 (10.9%)&quot;, y=&quot;Dimension 2 (9.6%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())

kmeans_3_pca </code></pre>
<p><img src="/blogs/scorsese_files/figure-html/PCA_visualization_with_different_k-1.png" width="1440" /></p>
<pre class="r"><code>#FOR K=4
kmeans_4_pca &lt;- fviz_cluster(model_kmeans_4clusters, userdata_sel, palette = &quot;Set2&quot;, main=&quot;K-Means Cluster Plot Across Two Principal Components, k = 4&quot;, x=&quot;Dimension 1 (10.9%)&quot;, y=&quot;Dimension 2 (9.6%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())

kmeans_4_pca </code></pre>
<p><img src="/blogs/scorsese_files/figure-html/PCA_visualization_with_different_k-2.png" width="1440" /></p>
<pre class="r"><code>#FOR K=5
kmeans_5_pca &lt;- fviz_cluster(model_kmeans_5clusters, userdata_sel, palette = &quot;Set2&quot;, main=&quot;K-Means Cluster Plot Across Two Principal Components, k = 5&quot;, x=&quot;Dimension 1 (10.9%)&quot;, y=&quot;Dimension 2 (9.6%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())

kmeans_5_pca</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/PCA_visualization_with_different_k-3.png" width="1440" /></p>
<pre class="r"><code>kmeans_3_pca+labs(title= &quot;K-Means Cluster Plot Across Two PCs&quot;,subtitle=&quot;k=3&quot;) + kmeans_4_pca+labs(title=&quot;&quot;,subtitle=&quot;k=4&quot;) + kmeans_5_pca+labs(title=&quot;&quot;,subtitle=&quot;k=5&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/PCA_visualization_with_different_k:all-1.png" width="1920" /></p>
<pre class="r"><code>#Plot centers

cluster_centers3&lt;-data.frame(cluster=as.factor(c(1:3)),model_kmeans_3clusters$centers)
cluster_centers4&lt;-data.frame(cluster=as.factor(c(1:4)),model_kmeans_4clusters$centers)
cluster_centers5&lt;-data.frame(cluster=as.factor(c(1:5)),model_kmeans_5clusters$centers)

#transpose data frames
cluster_centers3_t&lt;-cluster_centers3 %&gt;% gather(variable,value,-cluster,factor_key = TRUE)
cluster_centers4_t&lt;-cluster_centers4 %&gt;% gather(variable,value,-cluster,factor_key = TRUE)
cluster_centers5_t&lt;-cluster_centers5 %&gt;% gather(variable,value,-cluster,factor_key = TRUE)

#FOR K=3

kmeans3_centers &lt;- ggplot(cluster_centers3_t, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=&quot;K-means Cluster Centers, k=3&quot;, x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

kmeans3_centers</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/centers_for_different_k-1.png" width="1440" /></p>
<pre class="r"><code>#FOR K=4

kmeans4_centers &lt;- ggplot(cluster_centers4_t, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=&quot;K-means Cluster Centers, k=4&quot;, x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

kmeans4_centers</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/centers_for_different_3-1.png" width="1440" /></p>
<pre class="r"><code>#FOR K=5

kmeans5_centers &lt;- ggplot(cluster_centers5_t, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=&quot;K-means Cluster Centers, k=5&quot;, x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

kmeans5_centers</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/centers_for_different_5-1.png" width="1440" /></p>
<pre class="r"><code>kmeans3_centers/ kmeans4_centers / kmeans5_centers</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/centers_for_different_all-1.png" width="1440" /></p>
<p>Based on the graphs above, k = 3 seems to be more plausible, since at this level clusters are less overlapped in PCA visualization, and the cluster centers are more clearly distinguished than for either k = 4 or 5.</p>
<p>In all three cases, I observe that cluster 2 (in the k=3 case) is very well distinguished, with a relatively stable size (181, 180, 182) when I vary k from 3 to 5. From the k-means (k=3) center graph I observe that these are the people who enjoy viewing in the afternoon, with a strong preference for children’s content and less preference for any other types of content. Furthermore, it seems that cluster 3 (in all examples) is relatively stable, broadly representing customers that tend to watch News and Weather shows. However, this cluster varies in terms of size as k increases from 3 to 5 (1603 vs 622 vs 813).</p>
</div>
</div>
<div id="comparing-results-of-different-clustering-algorithms" class="section level1">
<h1>Comparing results of different clustering algorithms</h1>
<div id="pam" class="section level2">
<h2>PAM</h2>
<p>I now fit a PAM model for the k-value I chose for k-means (k=3).</p>
<pre class="r"><code>set.seed(1)

k=3

k3_pam &lt;-eclust(userdata_sel, &quot;pam&quot;, k = k, graph = FALSE)

#let&#39;s see the cluster sizes
k3_pam$clusinfo </code></pre>
<pre><code>##      size  max_diss  av_diss diameter separation
## [1,] 1129 15.083548 3.857930 21.04954  0.8633371
## [2,] 1140  9.992613 2.958928 13.67993  0.8633371
## [3,] 1356 34.473659 4.282944 38.02091  0.8764573</code></pre>
<pre class="r"><code>userdata_sel_withClusters&lt;-mutate(userdata_sel, cluster = as.factor(k3_pam$cluster))

center_locations &lt;- userdata_sel_withClusters%&gt;% group_by(cluster) %&gt;% summarize_at(vars(total_Time:mean_percentage_viewed),mean)

#next I use gather to collect information together
xa2p&lt;- gather(center_locations, key = &quot;variable&quot;, value = &quot;value&quot;,-cluster,factor_key = TRUE)

#next I use ggplot to visualize centers
pamcenters&lt;-ggplot(xa2p, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=paste0(&quot;PAM Cluster Centers, k=&quot;,k), x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

pamcenters</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/PAM%20centers-1.png" width="1440" /></p>
<pre class="r"><code>#First generate a new data frame with cluster medoids and cluster numbers
cluster_medoids&lt;-data.frame(cluster=as.factor(c(1:k)),k3_pam$medoids)

#transpose this data frame
cluster_medoids_t&lt;-cluster_medoids %&gt;% gather(variable,value,-cluster,factor_key = TRUE)

#plot medoids
graphkmeans_3Pam&lt;-ggplot(cluster_medoids_t, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=paste0(&quot;PAM Medoids Cluster Centers, k=&quot;,k), x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

graphkmeans_3Pam</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/PAM_Medoids-1.png" width="1440" /></p>
<pre class="r"><code>pam_pca &lt;- fviz_cluster(k3_pam, userdata_sel, palette = &quot;Set2&quot;, main=&quot;PAM Cluster Plot Across Two Principal Components, k = 3&quot;, x=&quot;Dimension 1 (10.9%)&quot;, y=&quot;Dimension 2 (9.6%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())

pam_pca</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/PAM_PCA-1.png" width="960" /></p>
</div>
<div id="h-clustering" class="section level2">
<h2>H-Clustering</h2>
<p>I now also use Hierarchical clustering with the same k (k=3) as above, comparing both ‘average’ and ‘ward’ methods.</p>
<pre class="r"><code>set.seed(1)

res.dist &lt;- dist(userdata_sel, method = &quot;euclidean&quot;)

res.hc.avg &lt;-  hcut(res.dist, hc_method = &quot;average&quot;, k=3)
res.hc.wardD &lt;-  hcut(res.dist, hc_method = &quot;ward.D&quot;, k=3)</code></pre>
<pre class="r"><code>res.hc.avg$size</code></pre>
<pre><code>## [1] 3623    1    1</code></pre>
<pre class="r"><code>res.hc.wardD$size</code></pre>
<pre><code>## [1] 2605  849  171</code></pre>
<pre class="r"><code>plot(res.hc.avg,hang = -1, cex = 0.5)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/2methods_of_h-cluster-1.png" width="1152" /></p>
<pre class="r"><code>plot(res.hc.wardD,hang = -1, cex = 0.5)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/2methods_of_h-cluster-2.png" width="1152" /></p>
<p>Each method resulted in completely different distances between clusters. The ‘Average’ method results in significantly smaller distances compared to distances from ‘ward.D’ method. Further, there is a significant difference in terms of cluster sizes.</p>
<p>While the Average method resulted in one big cluster (3623) , and two small clusters consisting of only individual points, the ‘ward.D’ method produced more diversified clusters in terms of size (2605, 849, 171). It seems that one cluster may bear resemblance to a cluster from k=3 K-means clustering approach.</p>
<p><em>I now plot the centers of H-clusters and compare the results with both K-Means and PAM</em></p>
<pre class="r"><code>userdata_sel_withClusters&lt;-mutate(userdata_sel, cluster = as.factor(res.hc.avg$cluster))

center_locations &lt;- userdata_sel_withClusters%&gt;% group_by(cluster) %&gt;% summarize_at(vars(total_Time:mean_percentage_viewed),mean)

xa2&lt;- gather(center_locations, key = &quot;variable&quot;, value = &quot;value&quot;,-cluster,factor_key = TRUE)

#Next I use ggplot to visualize centers
hclust_avg_center&lt;-ggplot(xa2, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=paste0(&quot;Hierarchical (Average Method)  Cluster Centers, k=&quot;,k), x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

hclust_avg_center</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/h-clustering_average_centers-1.png" width="1440" /></p>
<pre class="r"><code>userdata_sel_withClusters&lt;-mutate(userdata_sel, cluster = as.factor(res.hc.wardD$cluster))

center_locations &lt;- userdata_sel_withClusters%&gt;% group_by(cluster) %&gt;% summarize_at(vars(total_Time:mean_percentage_viewed),mean)

xa2&lt;- gather(center_locations, key = &quot;variable&quot;, value = &quot;value&quot;,-cluster,factor_key = TRUE)

#Next I use ggplot to visualize centers
hclust_wardD_center&lt;-ggplot(xa2, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=paste0(&quot;Hierarchical (Ward Method) Cluster Centers, k=&quot;,k), x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

hclust_wardD_center</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/h-clustering_wardD_centers-1.png" width="1440" /></p>
<pre class="r"><code>#AVERAGE METHOD

hc_avg_pca &lt;- fviz_cluster(res.hc.avg, userdata_sel, palette = &quot;Set2&quot;, main=&quot;Hierarchical Cluster Plot (Average Method) Across Two Principal Components, k = 3&quot;, x=&quot;Dimension 1 (10.9%)&quot;, y=&quot;Dimension 2 (9.6%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())

hc_avg_pca</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/h-clustering_PCA-1.png" width="1440" /></p>
<pre class="r"><code>#WARD METHOD

hc_wardD_pca &lt;- fviz_cluster(res.hc.wardD, userdata_sel, palette = &quot;Set2&quot;, main=&quot;Hierarchical Cluster Plot (Ward Method) Across Two Principal Components, k = 3&quot;, x=&quot;Dimension 1 (10.9%)&quot;, y=&quot;Dimension 2 (9.6%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())

hc_wardD_pca</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/h-clustering_PCA-2.png" width="1440" /></p>
<pre class="r"><code>(kmeans3_centers + pamcenters) / (hclust_avg_center + hclust_wardD_center)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/compare3methods_PCA-1.png" width="2400" /></p>
<pre class="r"><code>(kmeans_3_pca + pam_pca) / (hc_avg_pca + hc_wardD_pca)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/compare_PCA-1.png" width="2400" /></p>
<blockquote>
<p>I now draw some conclusions on the basis of the methods employed above</p>
</blockquote>
<p>I can conclude that K-means clustering and Hierarchical Clustering with the Ward method perform better than both PAM and H-Clustering (Average) methods, with more distinguished cluster centers and less overlap between clusters in PCA visualizations.</p>
<p>According to the visualization of cluster centers, these two methods both identified a cluster of BBC iPlayer viewers with a strong preference for children’s content and afternoon viewing. Furthermore, by checking the cluster size, I notice that k-means and h-clustering with wardD have one cluster of similar size (181 in k-means vs 171 in h-clustering, Ward). Therefore, I can state with some certainty that I have found a stable and particularly well-distinguished cluster.</p>
<p>Further, though with less certainty, I can also distinguish a cluster of consumers that tend to watch news and weather shows. Although this cluster does not stand out in the K-means method, it is pronounced in the cluster center plots of both the PAM (where it is represented by Cluster 1) and Hierarchical (Ward) methods (represented by Cluster 2).</p>
</div>
</div>
<div id="subsample-check" class="section level1">
<h1>Subsample check</h1>
<p>I now seek to verify that the clusters I have identified are not spurious by dividing the data into two equal parts.</p>
<p>In order to achieve this, I use K-means clustering, fixing the number of clusters, in these two data sets separately.</p>
<p>My criterion will be that, if I obtain similar looking clusters, my conclusions are relatively robust.</p>
<pre class="r"><code>#the following code chunk splits the data into two. Replace ... with your data frame that contains the data

set.seed(1234)
train_test_split &lt;- initial_split(userdata_sel, prop = 0.5)
testing &lt;- testing(train_test_split) #50% of the data is set aside for testing
training &lt;- training(train_test_split) #50% of the data is set aside for training

#Fit k-means to each dataset and compare your results

train_kmeans_3clusters&lt;-eclust(training, &quot;kmeans&quot;, k = 3, nstart = 50, graph = FALSE)
test_kmeans_3clusters&lt;-eclust(testing, &quot;kmeans&quot;, k = 3, nstart = 50, graph = FALSE)</code></pre>
<pre class="r"><code>cluster_centers3_train&lt;-data.frame(cluster=as.factor(c(1:3)),train_kmeans_3clusters$centers)
cluster_centers3_test&lt;-data.frame(cluster=as.factor(c(1:3)),test_kmeans_3clusters$centers)

cluster_centers3_train_df&lt;-cluster_centers3_train %&gt;% gather(variable,value,-cluster,factor_key = TRUE)
cluster_centers3_test_df&lt;-cluster_centers3_test %&gt;% gather(variable,value,-cluster,factor_key = TRUE)

kmeans3_centers_train &lt;- ggplot(cluster_centers3_train_df, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=paste0(&quot;K-Means Cluster Centers (Training Data), k=&quot;,k), x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

kmeans3_centers_test &lt;- ggplot(cluster_centers3_test_df, aes(x = variable, y = value))+  
  
  #using a line plot
  geom_line(aes(color =cluster,group = cluster), linetype = &quot;dashed&quot;,size=1)+
  
  #with connected points
  geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),)+
  
  #and a relevant title and subtitle
  labs(subtitle = &quot;&quot;, title=paste0(&quot;K-Means Cluster Centers (Testing Data), k=&quot;,k), x=&quot;Variable&quot;,y=&quot;Value&quot;,color=&quot;Cluster&quot;)+ 
  
  #and a relevant theme
  theme_fivethirtyeight() + theme(axis.title = element_text()) +
  
  #and ensuring the variables are named appropriately
  scale_x_discrete(labels=c(&quot;Total Time&quot;,&quot;Weekend&quot;,&quot;Afternoon&quot;,&quot;Day&quot;,&quot;Evening&quot;,&quot;Night&quot;,&quot;News&quot;,&quot;Entertainment&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Factual&quot;,&quot;Music&quot;,&quot;Sport&quot;,&quot;Weather&quot;,&quot;Religious Ethics&quot;,&quot;Children&quot;,&quot;Learning&quot;,&quot;Mean % Viewed&quot;))

kmeans3_centers / kmeans3_centers_train / kmeans3_centers_test</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/out_of_sample_cluster_center_check-1.png" width="1920" /></p>
<pre class="r"><code>train_pca &lt;- fviz_cluster(train_kmeans_3clusters, training, palette = &quot;Set2&quot;, main=&quot;K-Means Cluster Plot (Training Data) Across Two PCs, k = 3&quot;, x=&quot;Dimension 1 (11.2%)&quot;, y=&quot;Dimension 2 (9.3%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())

test_pca &lt;- fviz_cluster(test_kmeans_3clusters, testing, palette = &quot;Set2&quot;, main=&quot;K-Means Cluster Plot (Testing Data) Across Two PCs, k = 3&quot;, x=&quot;Dimension 1 (10.8%)&quot;, y=&quot;Dimension 2 (9.9%)&quot;, legend.title=&quot;Cluster&quot;)+theme_fivethirtyeight() + theme(axis.title = element_text())


kmeans_3_pca + train_pca + test_pca</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/out_of_sample_PCA_check-1.png" width="2400" /></p>
<blockquote>
<p>Robustness Checks</p>
</blockquote>
<p>Based on the graphs above, I are more confident in my results regarding one specific cluster (children’s content viewers and afternoon viewing habit).</p>
<p>Since k-means clustering with k = 3 performs relatively consistently in terms of both cluster centers and PCA across the whole data set, the train set and the test set, my clusters seem robust, and thus that the clusters identified are substantive and not simply due to chance.</p>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<blockquote>
<p>The Clusters which are likely to be substantive user profiles</p>
</blockquote>
<p>I can confidently conclude that there is one outstanding cluster of users of the BBC’s iPlayer platform who widely consume children’s content and watch shows predominantly in the afternoon.</p>
<blockquote>
<p>Evaluation of optimal k</p>
</blockquote>
<p>I believe k = 3 is the right choice, given the simplicity constraint of k=3-5, since the cluster centers are most distinguished from one another across the observed variables and clusters have the least overlap when evaluated in terms of principal components.</p>
<p>my model’s consistent out-of-sample performance also supports this decision. However, as shown in the elbow chart and silhouette analysis, there is likely to be be around 13 underlying clusters in my data, however this number of clusters may not be particularly insightful, since there is likely to be substantial overlap, meaning that interpretibility is low.</p>
<blockquote>
<p>Assumptions these results are sensitive to, and how the robustness of results to differing assumptions could be evaluated</p>
</blockquote>
<p>I assume that my analysis is sensitive to outliers (which is reflected by differing results in my PAM analysis compared to the K-means approach), choice of variables (for example, I could generate different measures for engagement besides average percentage completion, which may encourage formation of differing clusters), as well as to sample sizes. In order to check the robustness of my analysis, I would need to gather bigger sample over longer time period, ensuring that it is randomly selected and representative of the population at large. Perhaps with more data I could be more confident in having identified the second cluster of users who characteristically watch news and weather shows, though at present this remains a potentially spurious cluster.</p>
<blockquote>
<p>Applications of this cluster identification</p>
</blockquote>
<p><strong>Content Recommendations Notifications to Boost Engagement</strong>
For example, if a viewer mainly watches Children’s shows, then the BBC iPlayer app should send them notifications of recommended Children’s and Learning shows during the afternoon, since this is the time when this “Family” viewer is most likely to watch. Similarly, if a viewer mainly watches daytime News and Weather shows, notifications should be sent in the morning and recommend Sports, cross-selling other genres that the “Daytime Viewer” profile has a tendency to consume.</p>
<p><strong>Targeted Advertisement to Boost Advertising Revenues</strong>
At present BBC iPlayer does not allow third party advertisements, however, if it did, then these distinct viewer profiles would prove invaluable for targeting. For example, if a viewer mostly watches news during the day, I can expect that they are either retired or unemployed and target them with services such as holidays, loans and insurance. Alternatively, viewers who watch dramas in the evening can be targeted with products such as alcohol, fast food, cosmetics and cars.</p>
</div>
<div id="iiii.-how-can-we-recommend-them-suitable-films-1" class="section level1">
<h1><strong>II/II. How Can We Recommend Them Suitable Films?</strong></h1>
<div id="loading-the-data" class="section level2">
<h2>Loading the Data</h2>
<pre class="r"><code>#I load the following two datasets - movies and their corresponding ratings
movie_data &lt;- fread(here::here(&#39;data&#39;,&#39;movies.csv&#39;),stringsAsFactors=FALSE)

rating_data &lt;- fread(input = here::here(&#39;data&#39;,&#39;ratings.csv&#39;), select = c(1:3), nrows = 1e6)

#I use the following genre list for my data
list_genre &lt;- c(&quot;Action&quot;, &quot;Adventure&quot;, &quot;Animation&quot;, &quot;Children&quot;, 
                &quot;Comedy&quot;, &quot;Crime&quot;,&quot;Documentary&quot;, &quot;Drama&quot;, &quot;Fantasy&quot;,
                &quot;Film-Noir&quot;, &quot;Horror&quot;, &quot;Musical&quot;, &quot;Mystery&quot;,&quot;Romance&quot;,
                &quot;Sci-Fi&quot;, &quot;Thriller&quot;, &quot;War&quot;, &quot;Western&quot;)</code></pre>
</div>
<div id="cleaning-the-data" class="section level2">
<h2>Cleaning the Data</h2>
<div id="cleaning-movies-data" class="section level3">
<h3>Cleaning Movies Data</h3>
<pre class="r"><code>#I clean up those movies that appear more than once in the movie_data dataframe

#First, I see which movies appear more than once and save them as a vector

repeatMovies &lt;- movie_data %&gt;% 
  group_by(title) %&gt;% 
  
  #I count appearance of each movie
  summarise(n = n()) %&gt;% 
  
  #I filter for movies that appear more than once
  filter(n &gt; 1) %&gt;% 
  
  #I get the titles of those movies that appear more than once as a vector
  pull(title)

#I view the duplicated titles
repeatMovies

#I now initialize the variable that stores the rows I have to remove later (the duplicates)
removeRows &lt;- integer()

#For each movie that is duplicated, I run the following code...

for(i in repeatMovies){
  
  ### REMOVE DUPLICATES IN MOVIES DATA ###
  
  #I get the indices of the rows where the current movie (with title i) is stored 
  #(this will be more than 1 row, since the movie is duplicated!)
  repeatMovieLoc &lt;- which(movie_data$title == i)
  
  #I collapse the genres assigned to the duplicated movies, separated by |
  #With this line of code, I paste them both together as &quot;Action|Drama|Drama&quot;
  tempGenre &lt;- paste(movie_data$genres[repeatMovieLoc], collapse=&quot;|&quot;)
  
  #I now split that string up again, and take only the unique values
  tempGenre &lt;- paste(unique(unlist(strsplit(tempGenre, split = &quot;\\|&quot;)[[1]])), collapse = &quot;|&quot;)
  
  #In this line, I bring the new genre string to the first appearance of the movie in the dataframe
  movie_data$genres[repeatMovieLoc[1]] &lt;- tempGenre
  
  #I then add the duplicate entry (row 6271) to the list of rows I want to remove later
  removeRows &lt;- c(removeRows, repeatMovieLoc[-1])
  
  ### REMOVE DUPLICATES IN RATINGS DATA ###

  #I now search for all the entries in the rating_data dataframe that refer to duplicate movies in movie_data
  repeatMovieIdLoc &lt;- which(rating_data$movieId %in% movie_data$movieId[removeRows])
  
  #With this information, I change the movieId to the movieId that remains in my movie_data dataframe,
  rating_data$movieId[repeatMovieIdLoc] &lt;- movie_data$movieId[repeatMovieLoc[1]]
}

#Finally, I remove the duplicate rows in the movie_data dataframe
movie_data &lt;- movie_data[-removeRows,]</code></pre>
</div>
<div id="cleaning-ratings-data" class="section level3">
<h3>Cleaning Ratings Data</h3>
<pre class="r"><code>#First, I see which users rated a movie more than once and save them as a vector

repeatRatings &lt;- rating_data %&gt;% 
  group_by(movieId,userId) %&gt;% 
  
  #I count appearance of each movie
  summarise(n = n()) %&gt;% 
  
  #I filter for movies that appear more than once
  filter(n &gt; 1) %&gt;% 
  
#I pull the duplicated userId movieId combinations
pull(userId,movieId)

#I view the duplicated userId movieId combinations
repeatMovies

#Since I have identified duplicates where the same user has rated a movie multiple times, I only take the best rating

#Here, I group by userId and movieId, and only take the highest (maximum) rating

#This means I end up with a dataframe where there is (at most) 1 user rating for a movie
#This constraint is necessary to build up a proper user-movie-interaction-matrix

rating_data &lt;- rating_data %&gt;% 
  group_by(userId, movieId) %&gt;% 
  summarise(rating = max(rating)) %&gt;% 
  ungroup()

#Further, I identify movies not rated by any user, which I are safe to ignore for now, since they shouldn&#39;t affect my recommendations
length(unique(rating_data$movieId))
length(unique(movie_data$movieId))
setdiff(unique(movie_data$movieId), unique(rating_data$movieId)) </code></pre>
</div>
<div id="checking-the-data-is-clean" class="section level3">
<h3>Checking the Data is Clean</h3>
<pre class="r"><code>#movie_data
skim(movie_data)
summary(movie_data)    
head(movie_data)

#rating_data
skim(movie_data)
summary(rating_data)   
head(rating_data)

#I ensure there are no duplicate rows in either dataframe, meaning my cleaning has been successful

dupes_movie&lt;-movie_data%&gt;%get_dupes(movieId,title)
dupes_movie

dupes_rating&lt;-rating_data%&gt;%get_dupes(userId,movieId)
dupes_rating</code></pre>
<pre class="r"><code>searchMatrix &lt;- movie_data %&gt;% 
  
  #I split the genres assigned to movies into multiple rows
  separate_rows(genres, sep = &quot;\\|&quot;) %&gt;% 
  mutate(values = 1) %&gt;% 
  
  #I then pivot the dataframe so that each genre is assigned its own column
  pivot_wider(names_from = genres, values_from = values) %&gt;% 
  replace(is.na(.), 0)

#Though I find that there are 22 movies with no genres listed, this is not problematic for the recommendation model, since it is purely collaborative, rather than content-based
searchMatrix %&gt;% 
  filter(`(no genres listed)`==1) %&gt;% 
  length()</code></pre>
<p>I find, as intended, that each row of the movie_data represents a unique movie, and each row of the rating_data represents a unique user-movie rating combination, representing the maximum rating that each user gave for any given movie.</p>
</div>
</div>
<div id="building-the-rating-matrix-user-movie-interactions-matrix" class="section level2">
<h2>Building the Rating Matrix (User-Movie-Interactions Matrix)</h2>
<pre class="r"><code>#A ratings matrix consists of userId&#39;s as rows and movieId&#39;s as columns
ratingMatrix &lt;- rating_data %&gt;% 
  arrange(movieId) %&gt;% 
  
  #I allocate each movie to its own column
  pivot_wider(names_from = movieId, values_from = rating) %&gt;% 
  arrange(userId) %&gt;% 
  
  #I drop userId
  select(-userId)

#I convert the dataframe created above into a matrix
ratingMatrix &lt;- as.matrix(ratingMatrix)

#I order the rows and columns so that those with the fewest NAs are furthest to the top left, which is crucial for selecting the most popular movies and most frequent users
ratingMatrix &lt;- ratingMatrix[order(rowSums(is.na(ratingMatrix))),order(colSums(is.na(ratingMatrix)))]

#I perform a brief check to ensure that I still have same set of movies
setdiff(movie_data$movieId, as.integer(colnames(ratingMatrix)))

#I find that 18 movies present in the original movie_data are not present in the new ratingMatrix
#However, this data loss is unlikely to affect my analysis, so it is safe to ignore 

#I now convert the rating matrix into a recommenderlab sparse matrix
ratingMatrix &lt;- as(ratingMatrix, &quot;realRatingMatrix&quot;)
ratingMatrix</code></pre>
</div>
<div id="step-1---exploratory-analysis" class="section level2">
<h2><strong>STEP 1 - EXPLORATORY ANALYSIS</strong></h2>
</div>
<div id="preparing-the-data" class="section level2">
<h2>Preparing the Data</h2>
<div id="ratings-counts" class="section level3">
<h3>Ratings Counts</h3>
<pre class="r"><code>#I create a count of movie ratings
rating_data %&gt;% 
  group_by(rating) %&gt;% 
  summarise(n = n())

#I create a table with number of ratings per movie
movie_rating_count &lt;- rating_data %&gt;% 
  group_by(movieId) %&gt;% 
  summarise(ratings = n()) %&gt;% 
  left_join(movie_data, by = &quot;movieId&quot;) %&gt;% 
  select(movieId, title, ratings) %&gt;% 
  arrange(desc(ratings))

#I preview the table with the count of ratings per movie to ensure it has been defined correctly
head(movie_rating_count)

#I create a table with number of ratings per user
user_rating_count &lt;- rating_data %&gt;% 
  group_by(userId) %&gt;% 
  summarise(ratings = n()) %&gt;% 
  arrange(desc(ratings))

#I preview the table with the count of ratings per movie to ensure it has been defined correctly
head(user_rating_count)</code></pre>
</div>
<div id="selecting-only-top-100-most-popular-movies-and-top-100-most-frequent-users" class="section level3">
<h3>Selecting only Top 100 Most Popular Movies and Top 100 Most Frequent Users</h3>
<pre class="r"><code>#I only take the 100 most popular movies and 100 most popular users

top100_movies_users_ratings &lt;- ratingMatrix[1:100,1:100]

#I now have only the top 100 most popular movies and top 100 most frequent users in my matrix, containing 6223 ratings
top100_movies_users_ratings</code></pre>
<pre class="r"><code>#I build a plot which visualises this &quot;Top100&quot; user-movie-interaction-matrix (ratingMatrix)
image(top100_movies_users_ratings, axes = FALSE, main = &quot;Heatmap of the 100 Most Popular Movies and 100 Most Frequent Users&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/plotting_top100movies_top100users-1.png" width="672" /></p>
<pre class="r"><code>#Since this is relatively difficult to understand visually, I showcase just top top 25 most popular movies and top 25 most frequent users

image(top100_movies_users_ratings[1:25,1:25], axes = FALSE, main = &quot;Heatmap of the 25 Most Popular Movies and 25 Most Frequent Users&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/plotting_top100movies_top100users-2.png" width="672" /></p>
<p>The heatmap demonstrating ratings of the top 25 most frequent users and top 25 most popular movies is predominantly very dark, which means that the top users, who have rated a lot of movies, really like the movies that are rated most often.</p>
</div>
</div>
<div id="how-similar-the-100-most-popular-movies-are-to-eachother" class="section level2">
<h2>How similar the 100 most popular movies are to eachother</h2>
<pre class="r"><code>#I produce a matrix which calculates how similar movies are to one another
movie_similarity &lt;- similarity(ratingMatrix[, 1:100],
                               method = &quot;cosine&quot;,
                               which = &quot;items&quot;)

#I also add diag(100), since the similarity of a movie with itself is 1
movie_similarity &lt;- as.matrix(movie_similarity) + diag(100)

image(movie_similarity, main = &quot;Similarity of the 100 Most Popular Movies&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/similarity_100_popular_movies-1.png" width="960" /></p>
</div>
<div id="how-similar-the-100-most-frequent-users-are-to-eachother" class="section level2">
<h2>How similar the 100 most frequent users are to eachother</h2>
<pre class="r"><code>#I produce a matrix which calculates how similar users are to one another
user_similarity &lt;- similarity(ratingMatrix[1:100, ],
                              method = &quot;cosine&quot;,
                              which = &quot;users&quot;)

#I also add diag(100), since the similarity of a user with themselves is 1
user_similarity &lt;- as.matrix(user_similarity) + diag(100)

image(user_similarity, main = &quot;Similarity of the 100 Most Frequent Users&quot;)</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/similarity_100_frequent_users-1.png" width="960" /></p>
</div>
<div id="build-a-histogram-to-show-the-frequency-of-ratings-of-all-movies" class="section level2">
<h2>Build a Histogram to Show the Frequency of Ratings of All Movies</h2>
<pre class="r"><code>#I evaluate the count of number of ratings received by movies
movie_rating_count&lt;-rating_data %&gt;% 
  group_by(movieId) %&gt;% 
  count()

#I build a histogram showing the frequency of ratings of all movies
movie_rating_count %&gt;% 
  ggplot(aes(n)) + 
  geom_histogram(binwidth=5) +
  labs(title=&quot;Most Commonly, Movies Have 5 Ratings or Less&quot;,subtitle = &quot;Frequency of Movie Ratings Received&quot;,x=&quot;Number of Ratings per Movie&quot;,y=&quot;Count&quot;) + 
  theme_bw()+
  theme(axis.title = element_text())</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/histogram_frequency_movie_ratings-1.png" width="960" /></p>
</div>
<div id="build-a-histogram-to-show-the-frequency-of-ratings-of-all-users" class="section level2">
<h2>Build a Histogram to Show the Frequency of Ratings of All Users</h2>
<pre class="r"><code>#I evaluate the count of number of ratings given by Users
user_rating_count&lt;-rating_data %&gt;% 
  group_by(userId) %&gt;% 
  count()

#I build a histogram showing the frequency of ratings of all movies
user_rating_count %&gt;% 
  ggplot(aes(n)) + 
  geom_histogram(binwidth=10) +
  labs(title=&quot;Users Most Commonly Rate between 10 and 20 Movies&quot;,subtitle = &quot;Frequency of User Ratings Given&quot;,x=&quot;Number of Ratings per User&quot;,y=&quot;Count&quot;) + 
  theme_bw()+
  theme(axis.title = element_text())</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/histogram_frequency_user_ratings-1.png" width="960" /></p>
</div>
<div id="step-2---selecting-the-most-common-users-and-movies" class="section level2">
<h2><strong>STEP 2 - SELECTING THE MOST COMMON USERS AND MOVIES</strong></h2>
</div>
<div id="selecting-movies-with-more-than-m-ratings-and-users-who-have-given-more-than-n-ratings-where-m---c10-20-50-100-200-n---m" class="section level2">
<h2>Selecting movies with more than ‘m’ ratings and users who have given more than ‘n’ ratings, where m &lt;- c(10, 20, 50, 100, 200); n &lt;- m</h2>
<pre class="r"><code>#Since the rows (users) and columns (movies) of my ratingMatrix are already ranked in decreasing order of frequency/popularity, I can just slice the matrix

#movies rated more than 10 times and users who have rated at least 10 movies
top10_rate &lt;- ratingMatrix[rowCounts(ratingMatrix) &gt;= 10,colCounts(ratingMatrix) &gt;= 10] 
top10_rate &lt;- top10_rate[rowCounts(top10_rate) &gt; 5,] 

#movies rated more than 20 times and users who have rated at least 20 movies
top20_rate &lt;- ratingMatrix[rowCounts(ratingMatrix) &gt;= 20,colCounts(ratingMatrix) &gt;= 20] 
top20_rate &lt;- top20_rate[rowCounts(top20_rate) &gt; 5,] 

#movies rated more than 50 times and users who have rated at least 50 movies
top50_rate &lt;- ratingMatrix[rowCounts(ratingMatrix) &gt;= 50,colCounts(ratingMatrix) &gt;= 50] 
top50_rate &lt;- top50_rate[rowCounts(top50_rate) &gt; 5,] 

#movies rated more than 100 times and users who have rated at least 100 movies
top100_rate &lt;- ratingMatrix[rowCounts(ratingMatrix) &gt;= 100,colCounts(ratingMatrix) &gt;= 100] 
top100_rate &lt;- top100_rate[rowCounts(top100_rate) &gt; 5,] 

#movies rated more than 200 times and users who have rated at least 200 movies
top200_rate &lt;- ratingMatrix[rowCounts(ratingMatrix) &gt;= 200,colCounts(ratingMatrix) &gt;= 200]
top200_rate &lt;- top200_rate[rowCounts(top200_rate) &gt; 5,] </code></pre>
</div>
<div id="step-3---building-recommendation-systems" class="section level2">
<h2><strong>STEP 3 - BUILDING RECOMMENDATION SYSTEMS</strong></h2>
</div>
<div id="splitting-each-subset-into-test-and-training-data" class="section level2">
<h2>Splitting each subset into test and training data</h2>
<pre class="r"><code>#I set the seed to ensure reproducibility
set.seed(1)

#I intend to train my models on 80% of the data and test it on the remaining 20%
#I split the data into test and training data

top10_split &lt;- evaluationScheme(top10_rate, method=&quot;split&quot;, train=0.8, given=-5)
top20_split &lt;- evaluationScheme(top20_rate, method=&quot;split&quot;, train=0.8, given=-5)
top50_split &lt;- evaluationScheme(top50_rate, method=&quot;split&quot;, train=0.8, given=-5)
top100_split &lt;- evaluationScheme(top100_rate, method=&quot;split&quot;, train=0.8, given=-5)
top200_split &lt;- evaluationScheme(top200_rate, method=&quot;split&quot;, train=0.8, given=-5)</code></pre>
</div>
<div id="building-three-separate-recommendations-systems-item-based-cf-user-based-cf-and-a-model-based-cf-using-matrix-factorisation-for-each-of-the-values-of-n-and-m-5-versions-for-each-type-of-rs" class="section level2">
<h2>Building three separate recommendations systems (Item Based CF, User Based CF and a Model Based CF using Matrix Factorisation) for each of the values of ‘n’ and ‘m’ (5 versions for each type of RS)</h2>
<pre class="r"><code>#TOP10
top10_IBCF &lt;- Recommender(getData(top10_split, &quot;train&quot;), method = &quot;IBCF&quot;,param=list(k=25))

#TOP20
top20_IBCF &lt;- Recommender(getData(top20_split, &quot;train&quot;), method = &quot;IBCF&quot;,param=list(k=25))

#TOP50
top50_IBCF &lt;- Recommender(getData(top50_split, &quot;train&quot;), method = &quot;IBCF&quot;,param=list(k=25))

#TOP100
top100_IBCF &lt;- Recommender(getData(top100_split, &quot;train&quot;), method = &quot;IBCF&quot;,param=list(k=25))

#TOP200
top200_IBCF &lt;- Recommender(getData(top200_split, &quot;train&quot;), method = &quot;IBCF&quot;,param=list(k=25))</code></pre>
<pre class="r"><code>#TOP10
top10_UBCF &lt;- Recommender(getData(top10_split, &quot;train&quot;), method = &quot;UBCF&quot;,param=list(nn=25))

#TOP20
top20_UBCF &lt;- Recommender(getData(top20_split, &quot;train&quot;), method = &quot;UBCF&quot;,param=list(nn=25))

#TOP50
top50_UBCF &lt;- Recommender(getData(top50_split, &quot;train&quot;), method = &quot;UBCF&quot;,param=list(nn=25))

#TOP100
top100_UBCF &lt;- Recommender(getData(top100_split, &quot;train&quot;), method = &quot;UBCF&quot;,param=list(nn=25))

#TOP200
top200_UBCF &lt;- Recommender(getData(top200_split, &quot;train&quot;), method = &quot;UBCF&quot;,param=list(nn=25))</code></pre>
<pre class="r"><code>#TOP10
top10_LIBMF &lt;- Recommender(getData(top10_split, &quot;train&quot;), method = &quot;LIBMF&quot;,param = list(dim = 10))

#TOP20
top20_LIBMF &lt;- Recommender(getData(top20_split, &quot;train&quot;), method = &quot;LIBMF&quot;,param = list(dim = 10))

#TOP50
top50_LIBMF &lt;- Recommender(getData(top50_split, &quot;train&quot;), method = &quot;LIBMF&quot;,param = list(dim = 10))

#TOP100
top100_LIBMF &lt;- Recommender(getData(top100_split, &quot;train&quot;), method = &quot;LIBMF&quot;,param = list(dim = 10))

#TOP200
top200_LIBMF &lt;- Recommender(getData(top200_split, &quot;train&quot;), method = &quot;LIBMF&quot;,param = list(dim = 10))</code></pre>
</div>
<div id="step-4---computing-rmse-of-each-model-and-combination-of-m-x-n" class="section level2">
<h2><strong>STEP 4 - COMPUTING RMSE OF EACH MODEL AND COMBINATION OF [M x N]</strong></h2>
</div>
<div id="assessing-the-performance-of-the-above-models-for-different-number-of-items-and-users.-remember-cf-methods-are-most-appropriate-for-high-volume-items-often-watched-movies-and-frequent-users." class="section level2">
<h2>Assessing the performance of the above models for different number of ‘items’ and ‘users’. Remember CF methods are most appropriate for high volume items (often watched movies) and frequent users.</h2>
<pre class="r"><code>set.seed(1)

#TOP10

#I compute predicted ratings by giving the model the known part of the test data 
#(i.e. the data of the users for all but 5 movies for each user)
top10_IBCF_predict &lt;- predict(top10_IBCF, getData(top10_split, &quot;known&quot;), type=&quot;ratings&quot;)

#Finally, I can calculate RMSE between the predictions and the unknown part of the test data 
#(i.e. for the 5 movies that were held out)
top10_IBCF_RMSE &lt;- calcPredictionAccuracy(top10_IBCF_predict, getData(top10_split, &quot;unknown&quot;))[1]

#TOP20
top20_IBCF_predict &lt;- predict(top20_IBCF, getData(top20_split, &quot;known&quot;), type=&quot;ratings&quot;)
top20_IBCF_RMSE &lt;- calcPredictionAccuracy(top20_IBCF_predict, getData(top20_split, &quot;unknown&quot;))[1]

#TOP50
top50_IBCF_predict &lt;- predict(top50_IBCF, getData(top50_split, &quot;known&quot;), type=&quot;ratings&quot;)
top50_IBCF_RMSE &lt;- calcPredictionAccuracy(top50_IBCF_predict, getData(top50_split, &quot;unknown&quot;))[1]

#TOP100
top100_IBCF_predict &lt;- predict(top100_IBCF, getData(top100_split, &quot;known&quot;), type=&quot;ratings&quot;)
top100_IBCF_RMSE &lt;- calcPredictionAccuracy(top100_IBCF_predict, getData(top100_split, &quot;unknown&quot;))[1]

#TOP200
top200_IBCF_predict &lt;- predict(top200_IBCF, getData(top200_split, &quot;known&quot;), type=&quot;ratings&quot;)
top200_IBCF_RMSE &lt;- calcPredictionAccuracy(top200_IBCF_predict, getData(top200_split, &quot;unknown&quot;))[1]</code></pre>
<pre class="r"><code>set.seed(1)

#TOP10
top10_UBCF_predict &lt;- predict(top10_UBCF, getData(top10_split, &quot;known&quot;), type=&quot;ratings&quot;)
top10_UBCF_RMSE &lt;- calcPredictionAccuracy(top10_UBCF_predict, getData(top10_split, &quot;unknown&quot;))[1]

#TOP20
top20_UBCF_predict &lt;- predict(top20_UBCF, getData(top20_split, &quot;known&quot;), type=&quot;ratings&quot;)
top20_UBCF_RMSE &lt;- calcPredictionAccuracy(top20_UBCF_predict, getData(top20_split, &quot;unknown&quot;))[1]

#TOP50
top50_UBCF_predict &lt;- predict(top50_UBCF, getData(top50_split, &quot;known&quot;), type=&quot;ratings&quot;)
top50_UBCF_RMSE &lt;- calcPredictionAccuracy(top50_UBCF_predict, getData(top50_split, &quot;unknown&quot;))[1]

#TOP100
top100_UBCF_predict &lt;- predict(top100_UBCF, getData(top100_split, &quot;known&quot;), type=&quot;ratings&quot;)
top100_UBCF_RMSE &lt;- calcPredictionAccuracy(top100_UBCF_predict, getData(top100_split, &quot;unknown&quot;))[1]

#TOP200
top200_UBCF_predict &lt;- predict(top200_UBCF, getData(top200_split, &quot;known&quot;), type=&quot;ratings&quot;)
top200_UBCF_RMSE &lt;- calcPredictionAccuracy(top200_UBCF_predict, getData(top200_split, &quot;unknown&quot;))[1]</code></pre>
<pre class="r"><code>set.seed(1)

#TOP10
top10_LIBMF_predict &lt;- predict(top10_LIBMF, getData(top10_split, &quot;known&quot;), type=&quot;ratings&quot;)
top10_LIBMF_RMSE &lt;- calcPredictionAccuracy(top10_LIBMF_predict, getData(top10_split, &quot;unknown&quot;))[1]

#TOP20
top20_LIBMF_predict &lt;- predict(top20_LIBMF, getData(top20_split, &quot;known&quot;), type=&quot;ratings&quot;)
top20_LIBMF_RMSE &lt;- calcPredictionAccuracy(top20_LIBMF_predict, getData(top20_split, &quot;unknown&quot;))[1]

#TOP50
top50_LIBMF_predict &lt;- predict(top50_LIBMF, getData(top50_split, &quot;known&quot;), type=&quot;ratings&quot;)
top50_LIBMF_RMSE &lt;- calcPredictionAccuracy(top50_LIBMF_predict, getData(top50_split, &quot;unknown&quot;))[1]

#TOP100
top100_LIBMF_predict &lt;- predict(top100_LIBMF, getData(top100_split, &quot;known&quot;), type=&quot;ratings&quot;)
top100_LIBMF_RMSE &lt;- calcPredictionAccuracy(top100_LIBMF_predict, getData(top100_split, &quot;unknown&quot;))[1]

#TOP200
top200_LIBMF_predict &lt;- predict(top200_LIBMF, getData(top200_split, &quot;known&quot;), type=&quot;ratings&quot;)
top200_LIBMF_RMSE &lt;- calcPredictionAccuracy(top200_LIBMF_predict, getData(top200_split, &quot;unknown&quot;))[1]</code></pre>
</div>
<div id="summary-data-frame-of-all-the-rmses" class="section level2">
<h2>Summary Data Frame of All the RMSEs</h2>
<pre class="r"><code>RMSE_data &lt;- data.frame(&quot;number_ratings&quot;=c(10,20,50,100,200),&quot;IBCF&quot;=c(top10_IBCF_RMSE,top20_IBCF_RMSE,top50_IBCF_RMSE,top100_IBCF_RMSE,top200_IBCF_RMSE),&quot;UBCF&quot;=c(top10_UBCF_RMSE,top20_UBCF_RMSE,top50_UBCF_RMSE,top100_UBCF_RMSE,top200_UBCF_RMSE),&quot;LIBMF&quot;=c(top10_LIBMF_RMSE,top20_LIBMF_RMSE,top50_LIBMF_RMSE,top100_LIBMF_RMSE,top200_LIBMF_RMSE))

RMSE_dataround &lt;- round(RMSE_data,3)</code></pre>
<pre class="r"><code>RMSE_table &lt;- kable(RMSE_dataround, col.names=c(&quot;Number of Ratings Per User and Movie&quot;,&quot;IBCF&quot;,&quot;UBCF&quot;,&quot;LIBMF&quot;)) %&gt;% 
  kable_material_dark() %&gt;% 
  add_header_above(c(&quot; &quot; = 1,&quot;RMSE&quot;=3))

RMSE_table</code></pre>
</div>
<div id="step-5---reporting-plots-showing-how-rmse-changes-given-the-model-and-values-of-n-and-m" class="section level2">
<h2><strong>STEP 5 - REPORTING PLOTS SHOWING HOW RMSE CHANGES GIVEN THE MODEL AND VALUES OF ‘N’ AND ‘M’</strong></h2>
</div>
<div id="your-x-axis-should-be-values-of-nm-and-your-y-axis-should-be-rmse" class="section level2">
<h2>Your X-axis should be values of n,m and your y-axis should be RMSE</h2>
<pre class="r"><code>RMSE_data_longer&lt;- pivot_longer(RMSE_data,cols=2:4,names_to=&quot;recommendationSystem&quot;,values_to=&quot;RMSE&quot;)

ggplot(RMSE_data_longer, aes(x=number_ratings,y=RMSE,colour=recommendationSystem))+
  geom_line(size=0.5)+
  theme_solarized_2(light = FALSE) + 
  scale_colour_solarized(&quot;blue&quot;)+
  labs(title=&quot;Recommendation Performance Improves When Trained on More Common Users and Movies&quot;, subtitle=&quot;RMSE for 3 Model Specifications, Trained on 5 Data Subsets&quot;,x=&quot;Minimum Number of Reviews Given by Users and Received by Movies&quot;, y=&quot;Root Mean Square Error (RMSE)&quot;,col=&quot;Recommendation System&quot;)+
  theme(strip.text=element_text(colour=&quot;white&quot;))+
  theme(axis.text=element_text(colour=&quot;white&quot;)) +
  theme(axis.title.y=element_text(colour=&quot;white&quot;))+
  theme(axis.title.x=element_text(colour=&quot;white&quot;))+
  theme(plot.title=element_text(colour=&quot;white&quot;))+
  theme(legend.text=element_text(colour=&quot;white&quot;))+
  theme(legend.title = element_text(colour=&quot;white&quot;))+
  theme(plot.subtitle=element_text(colour=&quot;white&quot;))+
  theme(legend.position=&quot;top&quot;)+
  scale_y_continuous(breaks=seq(0.70,1.55,0.05),limits=c(0.70,1.55))+
  scale_x_continuous(breaks=c(10,20,50,100,200),limits=c(0,200))+
  geom_point()</code></pre>
<p><img src="/blogs/scorsese_files/figure-html/RMSE_plot_3models_5samples-1.png" width="960" />
On the basis of the RMSE’s above, my recommended recommendation system (which minimises RMSE) is: LIBMF, trained on a subset of the data including only movies that have been rated at least 50 times, and users that have given at least 50 ratings.</p>
</div>
</div>
